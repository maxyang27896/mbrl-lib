{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import argparse\n",
    "import cv2\n",
    "import copy\n",
    "from collections import deque\n",
    "import gym\n",
    "import hydra.utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, shutil\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "from typing import Optional, Sequence, cast\n",
    "import torch.nn as nn\n",
    "\n",
    "import mbrl\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.third_party.pytorch_sac_pranz24 as pytorch_sac_pranz24\n",
    "import mbrl.util.math as math_util\n",
    "\n",
    "from mbrl.planning.sac_wrapper import SACAgent, SACAgentSb3\n",
    "from mbrl.third_party.sb3_sac.sac import SAC\n",
    "from mbrl.util.plot_and_save_push_data import plot_and_save_training, plot_and_save_push_plots, clear_and_create_dir\n",
    "from mbrl.util.eval_agent import eval_and_save_vid\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "# from tactile_gym.utils.general_utils import get_orn_diff, quaternion_multiply, get_inverse_quaternion\n",
    "\n",
    "\n",
    "from stable_baselines3.common.torch_layers import NatureCNN\n",
    "from tactile_gym.sb3_helpers.custom.custom_torch_layers import CustomCombinedExtractor, ImpalaCNN\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar  8 2021 17:26:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n",
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    }
   ],
   "source": [
    "# Load the environment \n",
    "algo_name = 'ppo'\n",
    "env_name = 'object_push-v0'\n",
    "rl_params, algo_params, augmentations = import_parameters(env_name, algo_name)\n",
    "\n",
    "rl_params[\"max_ep_len\"] = 1000    \n",
    "rl_params[\"env_modes\"][\"observation_mode\"] = \"tactile_pose_relative_data\"\n",
    "rl_params[\"env_modes\"][ 'control_mode'] = 'TCP_position_control'\n",
    "rl_params[\"env_modes\"]['movement_mode'] = 'TyRz'\n",
    "rl_params[\"env_modes\"]['traj_type'] = 'point'\n",
    "rl_params[\"env_modes\"]['task'] = \"goal_pos\"\n",
    "rl_params[\"env_modes\"]['planar_states'] = True\n",
    "rl_params[\"env_modes\"]['use_contact'] = True\n",
    "rl_params[\"env_modes\"]['terminate_early']  = True\n",
    "rl_params[\"env_modes\"]['terminate_terminate_early'] = True\n",
    "\n",
    "rl_params[\"env_modes\"]['rand_init_orn'] = True\n",
    "# rl_params[\"env_modes\"]['rand_init_pos_y'] = True\n",
    "# rl_params[\"env_modes\"]['rand_obj_mass'] = True\n",
    "\n",
    "rl_params[\"env_modes\"]['additional_reward_settings'] = 'john_guide_off_normal'\n",
    "rl_params[\"env_modes\"]['terminated_early_penalty'] =  0.0\n",
    "rl_params[\"env_modes\"]['reached_goal_reward'] = 0.0\n",
    "rl_params[\"env_modes\"]['max_no_contact_steps'] = 1000\n",
    "rl_params[\"env_modes\"]['max_tcp_to_obj_orn'] = 180/180 * np.pi\n",
    "rl_params[\"env_modes\"]['importance_obj_goal_pos'] = 1.0\n",
    "rl_params[\"env_modes\"]['importance_obj_goal_orn'] = 1.0\n",
    "rl_params[\"env_modes\"]['importance_tip_obj_orn'] = 1.0\n",
    "rl_params[\"env_modes\"][\"x_speed_ratio\"] = 5.0\n",
    "rl_params[\"env_modes\"][\"y_speed_ratio\"] = 5.0\n",
    "rl_params[\"env_modes\"][\"Rz_speed_ratio\"] = 5.0\n",
    "\n",
    "rl_params[\"env_modes\"]['mpc_goal_orn_update'] = True\n",
    "rl_params[\"env_modes\"]['goal_orn_update_freq'] = 'every_step'\n",
    "\n",
    "\n",
    "# set limits and goals\n",
    "TCP_lims = np.zeros(shape=(6, 2))\n",
    "TCP_lims[0, 0], TCP_lims[0, 1] = -0.1, 0.4  # x lims\n",
    "TCP_lims[1, 0], TCP_lims[1, 1] = -0.3, 0.3  # y lims\n",
    "TCP_lims[2, 0], TCP_lims[2, 1] = -0.0, 0.0  # z lims\n",
    "TCP_lims[3, 0], TCP_lims[3, 1] = -0.0, 0.0  # roll lims\n",
    "TCP_lims[4, 0], TCP_lims[4, 1] = -0.0, 0.0  # pitch lims\n",
    "TCP_lims[5, 0], TCP_lims[5, 1] = -180 * np.pi / 180, 180 * np.pi / 180  # yaw lims\n",
    "\n",
    "# goal parameter\n",
    "goal_edges = [(0, -1), (0, 1), (1, 0)] # Top bottom and stright\n",
    "# goal_edges = [(1, 0)]\n",
    "goal_x_max = np.float64(TCP_lims[0, 1] * 0.8).item()\n",
    "goal_x_min = 0.0 # np.float64(TCP_lims[0, 0] * 0.6).item()\n",
    "goal_y_max = np.float64(TCP_lims[1, 1] * 0.6).item()\n",
    "goal_y_min = np.float64(TCP_lims[1, 0] * 0.6).item()\n",
    "goal_ranges = [goal_x_min, goal_x_max, goal_y_min, goal_y_max]\n",
    "\n",
    "rl_params[\"env_modes\"]['tcp_lims'] = TCP_lims.tolist()\n",
    "rl_params[\"env_modes\"]['goal_edges'] = goal_edges\n",
    "rl_params[\"env_modes\"]['goal_ranges'] = goal_ranges\n",
    "\n",
    "env_kwargs={\n",
    "    'show_gui':False,\n",
    "    'show_tactile':False,\n",
    "    'states_stacked_len': 1,\n",
    "    'max_steps':rl_params[\"max_ep_len\"],\n",
    "    'image_size':rl_params[\"image_size\"],\n",
    "    'env_modes':rl_params[\"env_modes\"],\n",
    "}\n",
    "\n",
    "# training environment\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "history_len = env.states_stacked_len\n",
    "step_obs_shape = (obs_shape[-1], )\n",
    "step_act_shape = act_shape\n",
    "stacked_obs_shape = (obs_shape[-1] * history_len, )\n",
    "stacked_act_shape = (history_len * act_shape[-1], )\n",
    "goal_shape = env.get_goal_obs().shape\n",
    "\n",
    "# evaluation environment\n",
    "num_eval_episodes = 7\n",
    "eval_env_kwargs = copy.deepcopy(env_kwargs)\n",
    "eval_env_kwargs[\"env_modes\"]['eval_mode'] = True\n",
    "eval_env_kwargs[\"env_modes\"]['eval_num'] = num_eval_episodes\n",
    "eval_env = gym.make(env_name, **eval_env_kwargs)\n",
    "\n",
    "work_dir = os.path.join(os.getcwd(), 'trained_mbpo')\n",
    "clear_and_create_dir(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def quaternions_product(quaternion0, quaternion1):\n",
    "#     x0, y0, z0, w0 = torch.tensor_split(quaternion0, 4, dim=-1)\n",
    "#     x1, y1, z1, w1 = torch.tensor_split(quaternion1, 4, dim=-1)\n",
    "#     return torch.concat(\n",
    "#         (x1*w0 + y1*z0 - z1*y0 + w1*x0,\n",
    "#          -x1*z0 + y1*w0 + z1*x0 + w1*y0,\n",
    "#          x1*y0 - y1*x0 + z1*w0 + w1*z0,\n",
    "#          -x1*x0 - y1*y0 - z1*z0 + w1*w0),\n",
    "#         axis=-1)\n",
    "\n",
    "# def get_inverse_quaternion_batched(orn_batched):\n",
    "#     conj_orn = orn_batched.clone()\n",
    "#     conj_orn[:, :3] = -conj_orn[:, :3] \n",
    "#     norm_orn = torch.linalg.norm(orn_batched, axis=-1).unsqueeze(1)\n",
    "\n",
    "#     return conj_orn/norm_orn\n",
    "\n",
    "# def get_orn_diff_batched(orn_1, orn_2):\n",
    "#     \"\"\"\n",
    "#     Calculate the difference between two orientaion quaternion in the same frame \n",
    "#     of reference\n",
    "#     \"\"\"\n",
    "#     return quaternions_product(orn_1, get_inverse_quaternion_batched(orn_2))\n",
    "\n",
    "\n",
    "# env.update_goal_orn()\n",
    "# _, cur_obj_orn_workframe = env.get_obj_pos_workframe()\n",
    "# goal_orn = env.goal_orn_workframe\n",
    "\n",
    "# # Batch goals and obj orn\n",
    "# obj_orn_batch = torch.tensor(cur_obj_orn_workframe).repeat(2, 1).to(torch.float32)\n",
    "# goal_orn_batch = torch.tensor(goal_orn).repeat(2, 1).to(torch.float32)\n",
    "\n",
    "# print(env.get_goal_aware_tactile_pose_relative_obs()[-2:])\n",
    "# print(get_orn_diff(cur_obj_orn_workframe, goal_orn))\n",
    "# print(get_orn_diff_batched(obj_orn_batch, goal_orn_batch))\n",
    "\n",
    "# # print(quaternion_multiply(cur_obj_orn_workframe, goal_orn))\n",
    "# # print(quat_multiply(obj_orn_batch, goal_orn_batch))\n",
    "\n",
    "# # print(get_inverse_quaternion(np.array(cur_obj_orn_workframe)))\n",
    "# # print(get_inverse_quaternion_batched(obj_orn_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1000\n",
    "initial_buffer_size = 10000\n",
    "trial_length = env._max_steps\n",
    "buffer_size = num_trials * trial_length\n",
    "\n",
    "# cfg_dict = {\n",
    "#     # dynamics model configuration\n",
    "#     \"dynamics_model\": {\n",
    "#         \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "#         \"device\": device,\n",
    "#         \"num_layers\": 3,\n",
    "#         \"ensemble_size\": 5,\n",
    "#         \"hid_size\": 200,\n",
    "#         \"in_size\": \"???\",\n",
    "#         \"out_size\": \"???\",\n",
    "#         \"deterministic\": False,\n",
    "#         \"propagation_method\": \"fixed_model\",\n",
    "#         \"learn_logvar_bounds\": False,\n",
    "#         # can also configure activation function for GaussianMLP\n",
    "#         # \"activation_fn_cfg\": {\n",
    "#         #     \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "#         #     \"negative_slope\": 0.01\n",
    "#         # }\n",
    "#         \"activation_fn_cfg\": {\n",
    "#             \"_target_\": \"torch.nn.SiLU\",\n",
    "#         }\n",
    "#     },\n",
    "#     # options for training the dynamics model\n",
    "#     \"algorithm\": {\n",
    "#         \"learned_rewards\": False,\n",
    "#         \"target_is_delta\": True,\n",
    "#         \"normalize\": True,\n",
    "#         \"target_normalize\": True,\n",
    "#         \"dataset_size\": buffer_size,\n",
    "#         \"initial_dataset_size\": initial_buffer_size,\n",
    "#         \"using_history_of_obs\": True,\n",
    "#         \"sac_samples_action\": True,\n",
    "#         \"real_data_ratio\": 0.0,\n",
    "#     },\n",
    "#     # these are experiment specific options\n",
    "#     \"overrides\": {\n",
    "#         \"trial_length\": trial_length,\n",
    "#         \"epoch_length\": trial_length,\n",
    "#         \"num_steps\": num_trials * trial_length,\n",
    "#         \"patience\": 25,\n",
    "#         \"num_epochs_train_model\": 25,\n",
    "#         \"model_lr\": 0.0005,\n",
    "#         \"model_wd\": 0.0001,\n",
    "#         \"model_batch_size\": 32,\n",
    "#         \"validation_ratio\": 0.0,\n",
    "#         \"freq_train_model\": 500,\n",
    "#         \"effective_model_rollouts_per_step\": 200,\n",
    "#         \"rollout_schedule\": [10, 50, 1, 20],\n",
    "#         \"num_sac_updates_per_step\": 20,\n",
    "#         \"sac_updates_every_steps\": 1,\n",
    "#         \"num_epochs_to_retain_sac_buffer\": 1,\n",
    "#         \"sac_batch_size\": 512,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# agent_cfg_dict = {\n",
    "#     \"_target_\": \"mbrl.third_party.pytorch_sac_pranz24.sac.SAC\",\n",
    "#     \"num_inputs\": \"???\",\n",
    "#     \"action_space\": {\n",
    "#         \"_target_\": \"gym.spaces.Box\",\n",
    "#         \"low\": \"???\",\n",
    "#         \"high\": \"???\",\n",
    "#         \"shape\": \"???\",\n",
    "#     },\n",
    "\n",
    "#     \"args\": {\n",
    "#         \"gamma\": 0.99,\n",
    "#         \"tau\": 0.005,\n",
    "#         \"alpha\": 0.2,\n",
    "#         \"policy\": \"Gaussian\",\n",
    "#         \"target_update_interval\": 1,\n",
    "#         \"automatic_entropy_tuning\": False,\n",
    "#         \"target_entropy\": 0.1,\n",
    "#         \"hidden_size\": 256,\n",
    "#         \"device\": device,\n",
    "#         \"lr\": 0.0001,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": 5,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        \"learn_logvar_bounds\": False,\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        # \"activation_fn_cfg\": {\n",
    "        #     \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "        #     \"negative_slope\": 0.01\n",
    "        # }\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.SiLU\",\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "        \"target_normalize\": True,\n",
    "        \"dataset_size\": buffer_size,\n",
    "        \"initial_dataset_size\": initial_buffer_size,\n",
    "        \"using_history_of_obs\": True,\n",
    "        \"sac_samples_action\": True,\n",
    "        \"real_data_ratio\": 0.0,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"epoch_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"patience\": 5,\n",
    "        \"num_epochs_train_model\": None,\n",
    "        \"model_lr\": 0.001,\n",
    "        \"model_wd\": 0.00005,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.0,\n",
    "        \"freq_train_model\": 500,\n",
    "        \"effective_model_rollouts_per_step\": 100,\n",
    "        \"rollout_schedule\": [20, 100, 1, 20],\n",
    "        \"num_sac_updates_per_step\": 10,\n",
    "        \"sac_updates_every_steps\": 1,\n",
    "        \"num_epochs_to_retain_sac_buffer\": 1,\n",
    "        \"sac_batch_size\": 512,\n",
    "    }\n",
    "}\n",
    "\n",
    "agent_cfg_dict = {\n",
    "    \"_target_\": \"mbrl.third_party.pytorch_sac_pranz24.sac.SAC\",\n",
    "    \"num_inputs\": \"???\",\n",
    "    \"action_space\": {\n",
    "        \"_target_\": \"gym.spaces.Box\",\n",
    "        \"low\": \"???\",\n",
    "        \"high\": \"???\",\n",
    "        \"shape\": \"???\",\n",
    "    },\n",
    "\n",
    "    \"args\": {\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"alpha\": 0.05,\n",
    "        \"policy\": \"Gaussian\",\n",
    "        \"target_update_interval\": 1,\n",
    "        \"automatic_entropy_tuning\": False,\n",
    "        \"target_entropy\": 0.1,\n",
    "        \"hidden_size\": 256,\n",
    "        \"device\": device,\n",
    "        \"lr\": 0.0003,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sac_params = {\n",
    "#     # === net arch ===\n",
    "#     \"policy_kwargs\": {\n",
    "#         \"features_extractor_class\": CustomCombinedExtractor,\n",
    "#         \"features_extractor_kwargs\": {\n",
    "#             'cnn_base':NatureCNN,\n",
    "#             # 'cnn_base':ImpalaCNN,\n",
    "#             'cnn_output_dim':256,\n",
    "#             'mlp_extractor_net_arch':[64, 64],\n",
    "#         },\n",
    "#         \"net_arch\": dict(pi=[256, 256], qf=[256, 256]),\n",
    "#         \"activation_fn\": nn.Tanh,\n",
    "#     },\n",
    "\n",
    "#     # ==== rl params ====\n",
    "#     \"learning_rate\": 1e-4,\n",
    "#     \"buffer_size\": int(1e6),\n",
    "#     \"learning_starts\": 1e4,\n",
    "#     \"batch_size\": 512,\n",
    "#     \"tau\": 0.005,\n",
    "#     \"gamma\": 0.99,\n",
    "#     \"train_freq\": 1,\n",
    "#     \"gradient_steps\": 1,\n",
    "#     \"action_noise\": None,\n",
    "#     \"optimize_memory_usage\":False,\n",
    "#     \"ent_coef\": \"auto\",\n",
    "#     \"target_update_interval\": 1,\n",
    "#     \"target_entropy\": \"auto\",\n",
    "#     \"use_sde\": False,\n",
    "#     \"sde_sample_freq\": -1,\n",
    "#     \"use_sde_at_warmup\": False,\n",
    "# }\n",
    "\n",
    "# model = SAC(\n",
    "#     rl_params[\"policy\"],\n",
    "#     env,\n",
    "#     **sac_params,\n",
    "#     verbose=1,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# agent = SACAgentSb3(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    }
   ],
   "source": [
    "agent_cfg = omegaconf.OmegaConf.create(agent_cfg_dict)\n",
    "planning.complete_agent_cfg(env, agent_cfg)\n",
    "agent = SACAgent(\n",
    "    cast(pytorch_sac_pranz24.SAC, hydra.utils.instantiate(agent_cfg))\n",
    ")\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneDTransitionRewardModel(\n",
      "  (model): GaussianMLP(\n",
      "    (hidden_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=10, out_size=200, bias=True)\n",
      "        (1): SiLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=200, out_size=200, bias=True)\n",
      "        (1): SiLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=200, out_size=200, bias=True)\n",
      "        (1): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (mean_and_logvar): EnsembleLinearLayer(num_members=5, in_size=200, out_size=16, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dynamics_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianPolicy(\n",
      "  (linear1): Linear(in_features=8, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (mean_linear): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "QNetwork(\n",
      "  (linear1): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (linear4): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (linear5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (linear6): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent.sac_agent.policy)\n",
    "print(agent.sac_agent.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.update_goal_orn()\n",
    "# obs = env.get_observation()\n",
    "# goal_aware_obs = env.get_goal_aware_tactile_pose_relative_obs()\n",
    "# goal_pos_workframe = env.goal_pos_workframe\n",
    "\n",
    "# # Batch goals and obj orn\n",
    "# obs_batch = np.repeat(obs, repeats=2, axis=0)\n",
    "# goal_pos_workframe_batch = np.repeat(goal_pos_workframe[None, :], repeats=2, axis=0)\n",
    "\n",
    "# model_env.reset_batch_goals(2)\n",
    "# model_env.update_step_data(obs_batch)\n",
    "# agent_obs = model_env.get_agent_obs(obs_batch)\n",
    "# agent_obs_goal = model_env.get_agent_obs(obs_batch, goal_pos_workframe_batch)\n",
    "\n",
    "# # print(obs_batch)\n",
    "# # Should be the same \n",
    "# print(goal_aware_obs)\n",
    "# print(agent_obs)\n",
    "# print(agent_obs_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected:  10000\n"
     ]
    }
   ],
   "source": [
    "# -------------- Create initial overrides. dataset --------------\n",
    "replay_buffer = common_util.create_replay_buffer(\n",
    "    cfg,\n",
    "    stacked_obs_shape,\n",
    "    stacked_act_shape,\n",
    "    rng=rng,\n",
    "    next_obs_shape=(obs_shape[-1], ),\n",
    "    goal_shape=goal_shape,\n",
    ")\n",
    "\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    initial_buffer_size,\n",
    "    agent,\n",
    "    {\"sample\": True, \"batched\": False},\n",
    "    replay_buffer=replay_buffer,\n",
    "    stacking=cfg.algorithm.using_history_of_obs,\n",
    "    store_goals=True,\n",
    ")\n",
    "\n",
    "print(\"Data collected: \", initial_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_result_directory = os.path.join(work_dir, 'random_exploration_agent')\n",
    "# clear_and_create_dir(evaluation_result_directory)\n",
    "\n",
    "# avg_reward = eval_and_save_vid(\n",
    "#     env,\n",
    "#     agent,\n",
    "#     n_eval_episodes=10,\n",
    "#     trial_length=trial_length,\n",
    "#     save_and_plot_flag=True,\n",
    "#     save_vid=True,\n",
    "#     render=True,\n",
    "#     data_directory=evaluation_result_directory,\n",
    "#     print_ep_reward=True,\n",
    "#     agent_kwargs={\"sample\": True, \"batched\": False},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = replay_buffer.get_all()\n",
    "# print(data.obs.shape)\n",
    "# print(data.act.shape)\n",
    "# print(data.next_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env.goal_orn_workframe)\n",
    "# init_obs = env.get_observation()\n",
    "# obs = init_obs[-1]\n",
    "# batch_size = 3\n",
    "# model_env.reset_batch_goals(batch_size)\n",
    "# model_env.update_goal_orn(torch.from_numpy(obs).repeat(batch_size, 1).to(device))\n",
    "# print(model_env.goal_orn_workframe_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_model_and_populate_sac_buffer(\n",
    "    model_env: models.ModelEnvPushing,\n",
    "    replay_buffer: mbrl.util.ReplayBuffer,\n",
    "    agent: SACAgent,\n",
    "    sac_buffer: mbrl.util.ReplayBuffer,\n",
    "    sac_samples_action: bool,\n",
    "    rollout_horizon: int,\n",
    "    batch_size: int,\n",
    "):\n",
    "\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    initial_obs, initial_act, _, _, _, initial_goals  = cast(mbrl.types.TransitionBatch, batch).astuple()\n",
    "    # goal_pos_workframe = np.empty((batch_size, 3))\n",
    "    # goal_pos_workframe[:, 0:2] = initial_goals[:, 4:6]\n",
    "    model_state = model_env.reset(\n",
    "        initial_obs_batch=cast(np.ndarray, initial_obs),\n",
    "        return_as_np=True,\n",
    "    )\n",
    "    accum_dones = np.zeros(initial_obs.shape[0], dtype=bool)\n",
    "    obs = initial_obs\n",
    "    stacked_act = initial_act\n",
    "    model_env.reset_batch_goals(batch_size, sample_goals=True)\n",
    "    # model_env.reset_batch_goals(batch_size, sample_goals=False, goal_batch=goal_pos_workframe)\n",
    "    model_env.update_step_data(obs)\n",
    "    for i in range(rollout_horizon):\n",
    "        agent_obs = model_env.get_agent_obs(obs)\n",
    "        action = agent.act(agent_obs, action=None, sample=sac_samples_action, batched=True)\n",
    "        stacked_act = np.concatenate([stacked_act, action], axis=action.ndim - 1)\n",
    "        stacked_act = stacked_act[:, -initial_act.shape[-1]:]\n",
    "        pred_next_obs, pred_rewards, pred_dones, model_state = model_env.step(\n",
    "            stacked_act, model_state, sample=True\n",
    "        )\n",
    "        pred_next_obs = np.concatenate([obs, pred_next_obs], axis=pred_next_obs.ndim - 1)\n",
    "        pred_next_obs = pred_next_obs[:, -initial_obs.shape[-1]:]\n",
    "        agent_next_obs = model_env.get_agent_obs(pred_next_obs)\n",
    "        sac_buffer.add_batch(\n",
    "            agent_obs[~accum_dones],\n",
    "            action[~accum_dones],\n",
    "            agent_next_obs[~accum_dones],\n",
    "            pred_rewards[~accum_dones, 0],\n",
    "            pred_dones[~accum_dones, 0],\n",
    "        )\n",
    "        obs = pred_next_obs\n",
    "        accum_dones |= pred_dones.squeeze()\n",
    "\n",
    "\n",
    "def maybe_replace_sac_buffer(\n",
    "    sac_buffer: Optional[mbrl.util.ReplayBuffer],\n",
    "    obs_shape: Sequence[int],\n",
    "    act_shape: Sequence[int],\n",
    "    new_capacity: int,\n",
    "    seed: int,\n",
    ") -> mbrl.util.ReplayBuffer:\n",
    "    if sac_buffer is None or new_capacity != sac_buffer.capacity:\n",
    "        if sac_buffer is None:\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "        else:\n",
    "            rng = sac_buffer.rng\n",
    "        new_buffer = mbrl.util.ReplayBuffer(new_capacity, obs_shape, act_shape, rng=rng)\n",
    "        if sac_buffer is None:\n",
    "            return new_buffer\n",
    "        obs, action, next_obs, reward, done, _ = sac_buffer.get_all().astuple()\n",
    "        new_buffer.add_batch(obs, action, next_obs, reward, done)\n",
    "        return new_buffer\n",
    "    return sac_buffer\n",
    "\n",
    "\n",
    "# rollout_batch_size = (\n",
    "#     cfg.overrides.effective_model_rollouts_per_step * cfg.overrides.freq_train_model\n",
    "# )\n",
    "# trains_per_epoch = int(\n",
    "#     np.ceil(cfg.overrides.epoch_length / cfg.overrides.freq_train_model)\n",
    "# )\n",
    "# rollout_length = int(\n",
    "#     math_util.truncated_linear(\n",
    "#         *(cfg.overrides.rollout_schedule + [1])\n",
    "#     )\n",
    "# )\n",
    "# sac_buffer = None\n",
    "# sac_buffer_capacity = rollout_length * rollout_batch_size * trains_per_epoch\n",
    "# sac_buffer_capacity *= cfg.overrides.num_epochs_to_retain_sac_buffer\n",
    "# sac_buffer = maybe_replace_sac_buffer(\n",
    "#     sac_buffer, stacked_obs_shape, act_shape, sac_buffer_capacity, rng\n",
    "# )\n",
    "\n",
    "# # Batch all rollouts for the next freq_train_model steps together\n",
    "# rollout_model_and_populate_sac_buffer(\n",
    "#     model_env,\n",
    "#     replay_buffer,\n",
    "#     agent,\n",
    "#     sac_buffer,\n",
    "#     cfg.algorithm.sac_samples_action,\n",
    "#     rollout_length,\n",
    "#     rollout_batch_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obs = torch.randn(1, 8).to(device)\n",
    "# obs = np.random.randn(32, 8)\n",
    "# print(agent.sac_agent.sample_action(obs, batched=True, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_to_sac_buffer(replay_buffer, sac_obs_shape, sac_act_shape, rng):\n",
    "    new_sac_buffer = mbrl.util.ReplayBuffer(replay_buffer.num_stored, sac_obs_shape, sac_act_shape, rng=rng)\n",
    "    obs, action, next_obs, reward, done, _ = replay_buffer.get_all().astuple()\n",
    "\n",
    "    # Get next_obs and action states\n",
    "    next_obs = np.concatenate([obs[:, next_obs.shape[-1]:], next_obs], axis=1)\n",
    "    action = action[:, -sac_act_shape[-1]:]\n",
    "\n",
    "    # Fill new buffer\n",
    "    new_sac_buffer.add_batch(obs, action, next_obs, reward, done)\n",
    "\n",
    "    return new_sac_buffer\n",
    "\n",
    "# obs, action, next_obs, *_ = replay_buffer.get_all().astuple()\n",
    "# print(obs.shape)\n",
    "# print(next_obs.shape)\n",
    "# print(action.shape)\n",
    "# new_buffer = replay_to_sac_buffer(replay_buffer, stacked_obs_shape, act_shape, rng)\n",
    "# new_obs, new_action, new_next_obs, *_ = new_buffer.get_all().astuple()\n",
    "# print(new_obs.shape)\n",
    "# print(new_next_obs.shape)\n",
    "# print(new_action.shape)\n",
    "\n",
    "# print(obs[0])\n",
    "# print(new_obs[0])\n",
    "# print(action[0])\n",
    "# print(new_action[0])\n",
    "# print(next_obs[0])\n",
    "# print(new_next_obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_and_save_model_and_data(\n",
    "#     model,\n",
    "#     model_trainer,\n",
    "#     cfg,\n",
    "#     replay_buffer,\n",
    "#     work_dir = None,\n",
    "#     callback = None,    \n",
    "# ):\n",
    "#     dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats            \n",
    "#     dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "#         replay_buffer,\n",
    "#         batch_size=cfg.overrides.model_batch_size,\n",
    "#         val_ratio=cfg.overrides.validation_ratio,\n",
    "#         ensemble_size=len(dynamics_model),\n",
    "#         shuffle_each_epoch=True,\n",
    "#         bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "#     )\n",
    "\n",
    "#     model_trainer.train(\n",
    "#         dataset_train, \n",
    "#         dataset_val=dataset_val, \n",
    "#         num_epochs=cfg.overrides.num_epochs_train_model, \n",
    "#         patience=cfg.overrides.patience, \n",
    "#         callback=callback,\n",
    "#         silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. SAC buffer size: 100000. Real buffer size: 11000. Rollout length: 1. Steps: 999Average reward: -533.196698152695 \n",
      "Saving models to /home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/trained_mbpo/sac.pth\n",
      "Saved best model\n",
      "Epoch: 1. SAC buffer size: 100000. Real buffer size: 12000. Rollout length: 1. Steps: 1999Average reward: -382.02271662797244 \n",
      "Saving models to /home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/trained_mbpo/sac.pth\n",
      "Saved best model\n",
      "Epoch: 2. SAC buffer size: 100000. Real buffer size: 13000. Rollout length: 1. Steps: 2999Average reward: -602.4292650830384 \n",
      "Epoch: 3. SAC buffer size: 100000. Real buffer size: 14000. Rollout length: 1. Steps: 3999Average reward: -775.4466397417972 \n",
      "Epoch: 4. SAC buffer size: 100000. Real buffer size: 15000. Rollout length: 1. Steps: 4999Average reward: -390.95800131377524 \n",
      "Epoch: 5. SAC buffer size: 100000. Real buffer size: 16000. Rollout length: 1. Steps: 5999Average reward: -709.0977394630523 \n",
      "Epoch: 6. SAC buffer size: 100000. Real buffer size: 17000. Rollout length: 1. Steps: 6999Average reward: -553.3576312644619 \n",
      "Epoch: 7. SAC buffer size: 100000. Real buffer size: 18000. Rollout length: 1. Steps: 7999Average reward: -1123.4383503540046 \n",
      "Epoch: 8. SAC buffer size: 100000. Real buffer size: 19000. Rollout length: 1. Steps: 8999Average reward: -222.82604451421852 \n",
      "Saving models to /home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/trained_mbpo/sac.pth\n",
      "Saved best model\n",
      "Epoch: 9. SAC buffer size: 100000. Real buffer size: 20000. Rollout length: 1. Steps: 9999Average reward: -710.6971871419719 \n",
      "Epoch: 10. SAC buffer size: 100000. Real buffer size: 21000. Rollout length: 1. Steps: 10999Average reward: -933.7215087494072 \n",
      "Epoch: 11. SAC buffer size: 100000. Real buffer size: 22000. Rollout length: 1. Steps: 11999Average reward: -2114.94859404046 \n",
      "Epoch: 12. SAC buffer size: 100000. Real buffer size: 23000. Rollout length: 1. Steps: 12999Average reward: -654.4196195832635 \n",
      "Epoch: 13. SAC buffer size: 100000. Real buffer size: 24000. Rollout length: 1. Steps: 13999Average reward: -811.8226555520606 \n",
      "Epoch: 14. SAC buffer size: 100000. Real buffer size: 25000. Rollout length: 1. Steps: 14999Average reward: -1197.065132959777 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (512, 2)) of distribution Normal(loc: torch.Size([512, 2]), scale: torch.Size([512, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m     \u001b[39m# print(\"Agent training skipped. Buffer not big enough\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m  \u001b[39m# only update every once in a while\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m (\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m     qf1_loss,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m     qf2_loss,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=192'>193</a>\u001b[0m     policy_loss,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m     _,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=194'>195</a>\u001b[0m     _,\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=195'>196</a>\u001b[0m ) \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msac_agent\u001b[39m.\u001b[39;49mupdate_parameters(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m     which_buffer,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=197'>198</a>\u001b[0m     cfg\u001b[39m.\u001b[39;49moverrides\u001b[39m.\u001b[39;49msac_batch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=198'>199</a>\u001b[0m     updates_made,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=199'>200</a>\u001b[0m     logger\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=200'>201</a>\u001b[0m     reverse_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=201'>202</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39m# qf2_loss = 0\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=203'>204</a>\u001b[0m \u001b[39m# (\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=204'>205</a>\u001b[0m \u001b[39m#     qf1_loss,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=211'>212</a>\u001b[0m \u001b[39m#     cfg.overrides.sac_batch_size,\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=213'>214</a>\u001b[0m updates_made \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/third_party/pytorch_sac_pranz24/sac.py:98\u001b[0m, in \u001b[0;36mSAC.update_parameters\u001b[0;34m(self, memory, batch_size, updates, logger, reverse_mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m     mask_batch \u001b[39m=\u001b[39m mask_batch\u001b[39m.\u001b[39mlogical_not()\n\u001b[1;32m     97\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 98\u001b[0m     next_state_action, next_state_log_pi, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m     99\u001b[0m         next_state_batch\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     qf1_next_target, qf2_next_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_target(\n\u001b[1;32m    102\u001b[0m         next_state_batch, next_state_action\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m     min_qf_next_target \u001b[39m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m         torch\u001b[39m.\u001b[39mmin(qf1_next_target, qf2_next_target)\n\u001b[1;32m    106\u001b[0m         \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m next_state_log_pi\n\u001b[1;32m    107\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/third_party/pytorch_sac_pranz24/model.py:104\u001b[0m, in \u001b[0;36mGaussianPolicy.sample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    102\u001b[0m mean, log_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(state)\n\u001b[1;32m    103\u001b[0m std \u001b[39m=\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> 104\u001b[0m normal \u001b[39m=\u001b[39m Normal(mean, std)\n\u001b[1;32m    105\u001b[0m x_t \u001b[39m=\u001b[39m normal\u001b[39m.\u001b[39mrsample()  \u001b[39m# for reparameterization trick (mean + std * N(0,1))\u001b[39;00m\n\u001b[1;32m    106\u001b[0m y_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(x_t)\n",
      "File \u001b[0;32m~/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/torch/distributions/normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 50\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m constraint\u001b[39m.\u001b[39mcheck(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param))\u001b[39m.\u001b[39mall():\n\u001b[1;32m     63\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe parameter \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has invalid values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(param))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (512, 2)) of distribution Normal(loc: torch.Size([512, 2]), scale: torch.Size([512, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "def plot_and_save(y_data, x_data=None, title=None, xlabel=None, ylabel=None):\n",
    "\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(2):\n",
    "            \n",
    "            if (2 * i + j) == 5:\n",
    "                break\n",
    "            if not x_data:\n",
    "                ax[i, j].plot(y_data[2 * i + j])\n",
    "            else:   \n",
    "                ax[i, j].plot(x_data[2 * i + j], y_data[2 * i + j])\n",
    "            \n",
    "            if title:\n",
    "                ax[i, j].set_title(title[2 * i + j])\n",
    "            if xlabel:\n",
    "                ax[i, j].set_xlabel(xlabel[2 * i + j])\n",
    "            if ylabel:\n",
    "                ax[i, j].set_ylabel(ylabel[2 * i + j])\n",
    "\n",
    "    fig.savefig(os.path.join(work_dir, \"losses.png\"))\n",
    "    plt.close(fig)\n",
    "\n",
    "training_result_directory = os.path.join(work_dir, \"training_result\")\n",
    "data_columns = ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'tcp_Rz', 'contact_Rz', 'goal_x', 'goal_y', 'goal_Rz', 'rewards', 'contact', 'dones']\n",
    "\n",
    "# Save losses \n",
    "train_losses = [0.0]\n",
    "val_scores = [0.0]\n",
    "policy_losses = [0.0]\n",
    "qf1_losses = [0.0]\n",
    "qf2_losses = [0.0]\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --------------------- Training Loop ---------------------\n",
    "rollout_batch_size = (\n",
    "    cfg.overrides.effective_model_rollouts_per_step * cfg.overrides.freq_train_model\n",
    ")\n",
    "trains_per_epoch = int(\n",
    "    np.ceil(cfg.overrides.epoch_length / cfg.overrides.freq_train_model)\n",
    ")\n",
    "updates_made = 0\n",
    "env_steps = 0\n",
    "model_env = models.ModelEnvPushing(\n",
    "    env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator\n",
    ")\n",
    "model_trainer = models.ModelTrainer(\n",
    "    dynamics_model,\n",
    "    optim_lr=cfg.overrides.model_lr,\n",
    "    weight_decay=cfg.overrides.model_wd,\n",
    ")\n",
    "\n",
    "best_eval_reward = -np.inf\n",
    "epoch = 0\n",
    "sac_buffer = None\n",
    "all_train_rewards = [0]\n",
    "all_eval_rewards = [0]\n",
    "total_steps_train = [0]\n",
    "total_steps_eval = [0]\n",
    "goal_reached = [0]\n",
    "trial_push_result = []\n",
    "trial = 0\n",
    "while env_steps < cfg.overrides.num_steps:\n",
    "    rollout_length = int(\n",
    "        math_util.truncated_linear(\n",
    "            *(cfg.overrides.rollout_schedule + [epoch + 1])\n",
    "        )\n",
    "    )\n",
    "    sac_buffer_capacity = rollout_length * rollout_batch_size * trains_per_epoch\n",
    "    sac_buffer_capacity *= cfg.overrides.num_epochs_to_retain_sac_buffer\n",
    "    sac_buffer = maybe_replace_sac_buffer(\n",
    "        sac_buffer, stacked_obs_shape, act_shape, sac_buffer_capacity, rng\n",
    "    )\n",
    "    obs, done = None, False\n",
    "    for steps_epoch in range(cfg.overrides.epoch_length):\n",
    "        if steps_epoch == 0 or done:\n",
    "            if done:\n",
    "                # save goal reached data during training\n",
    "                if env.single_goal_reached:\n",
    "                    goal_reached.append(trial_reward)\n",
    "                else:\n",
    "                    goal_reached.append(0)\n",
    "\n",
    "                # Save data to csv and plot\n",
    "                all_train_rewards.append(trial_reward)\n",
    "                total_steps_train.append(steps_trial + total_steps_train[-1])\n",
    "                trial_time = time.time() - start_trial_time\n",
    "\n",
    "                # Save data to csv and plot\n",
    "                # trial_push_result = np.array(trial_push_result)\n",
    "                # plot_and_save_training(env, trial_push_result, trial, data_columns, training_result_directory)\n",
    "                trial_push_result = []\n",
    "\n",
    "                # Save and plot training curve \n",
    "                training_result = np.stack((total_steps_train[1:], all_train_rewards[1:]), axis=-1)\n",
    "                pd.DataFrame(training_result).to_csv(os.path.join(work_dir, \"{}_result.csv\".format(\"train_curve\")))\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                ax.plot(total_steps_train[1:], all_train_rewards[1:], 'bs-', total_steps_train[1:], goal_reached[1:], 'rs')\n",
    "                ax.set_xlabel(\"Samples\")\n",
    "                ax.set_ylabel(\"Trial reward\")\n",
    "                fig.savefig(os.path.join(work_dir, \"output_train.png\"))        \n",
    "                plt.close(fig)\n",
    "\n",
    "                trial += 1\n",
    "\n",
    "            obs, done = env.reset(), False\n",
    "            stacked_act = deque(np.zeros((env.states_stacked_len, *env.action_space.shape)), maxlen=env.states_stacked_len)\n",
    "            trial_reward = 0.0\n",
    "            trial_pb_steps = 0.0\n",
    "            steps_trial = 0\n",
    "            start_trial_time = time.time()\n",
    "\n",
    "            (tcp_pos_workframe, \n",
    "            tcp_rpy_workframe,\n",
    "            cur_obj_pos_workframe, \n",
    "            cur_obj_rpy_workframe) = env.get_obs_workframe()\n",
    "            trial_push_result.append(np.hstack([trial, \n",
    "                                                steps_trial, \n",
    "                                                trial_pb_steps,\n",
    "                                                tcp_pos_workframe, \n",
    "                                                cur_obj_pos_workframe, \n",
    "                                                tcp_rpy_workframe[2],\n",
    "                                                cur_obj_rpy_workframe[2],\n",
    "                                                env.goal_pos_workframe[0:2], \n",
    "                                                env.goal_rpy_workframe[2],\n",
    "                                                trial_reward, \n",
    "                                                False,\n",
    "                                                done]))\n",
    "                        \n",
    "\n",
    "        # --- Doing env step and adding to model dataset ---\n",
    "        next_obs, reward, done, info = common_util.step_env_and_add_to_buffer_stacked(\n",
    "            env, obs, agent, {}, replay_buffer, stacked_action=stacked_act, store_goals=True,\n",
    "        )\n",
    "\n",
    "        # --------------- Model Training -----------------\n",
    "        if (env_steps + 1) % cfg.overrides.freq_train_model == 0:\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=len(dynamics_model),\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "            )\n",
    "            # print(\"Training model...\")\n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=cfg.overrides.num_epochs_train_model, \n",
    "                patience=cfg.overrides.patience, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "            # print(\"Model training done.\")\n",
    "            \n",
    "            \n",
    "            # --------- Rollout new model and store imagined trajectories --------\n",
    "            # Batch all rollouts for the next freq_train_model steps together\n",
    "            rollout_model_and_populate_sac_buffer(\n",
    "                model_env,\n",
    "                replay_buffer,\n",
    "                agent,\n",
    "                sac_buffer,\n",
    "                cfg.algorithm.sac_samples_action,\n",
    "                rollout_length,\n",
    "                rollout_batch_size,\n",
    "            )\n",
    "\n",
    "        # --------------- Agent Training -----------------\n",
    "        # print(\"Training agent...\")\n",
    "        for _ in range(cfg.overrides.num_sac_updates_per_step):\n",
    "            use_real_data = rng.random() < cfg.algorithm.real_data_ratio\n",
    "            replay_sac_buffer = replay_to_sac_buffer(replay_buffer, stacked_obs_shape, act_shape, rng)\n",
    "            which_buffer = replay_sac_buffer if use_real_data else sac_buffer\n",
    "            if (env_steps + 1) % cfg.overrides.sac_updates_every_steps != 0 or len(\n",
    "                which_buffer\n",
    "            ) < cfg.overrides.sac_batch_size:\n",
    "                \n",
    "                qf1_loss = 0\n",
    "                qf2_loss = 0\n",
    "                policy_loss = 0\n",
    "                # print(\"Agent training skipped. Buffer not big enough\")\n",
    "                break  # only update every once in a while\n",
    "            (\n",
    "                qf1_loss,\n",
    "                qf2_loss,\n",
    "                policy_loss,\n",
    "                _,\n",
    "                _,\n",
    "            ) = agent.sac_agent.update_parameters(\n",
    "                which_buffer,\n",
    "                cfg.overrides.sac_batch_size,\n",
    "                updates_made,\n",
    "                logger=None,\n",
    "                reverse_mask=True,\n",
    "            )\n",
    "            # qf2_loss = 0\n",
    "            # (\n",
    "            #     qf1_loss,\n",
    "            #     policy_loss,\n",
    "            #     _,\n",
    "            #     _,\n",
    "            # ) = agent.sac_agent.train(\n",
    "            #     1,\n",
    "            #     which_buffer,\n",
    "            #     cfg.overrides.sac_batch_size,\n",
    "            # )\n",
    "            updates_made += 1\n",
    "        # print(\"Agent training done.\")\n",
    "\n",
    "        qf1_losses.append(qf1_loss)\n",
    "        qf2_losses.append(qf2_loss)\n",
    "        policy_losses.append(policy_loss)\n",
    "\n",
    "        # ------ Epoch ended (evaluate and save model) ------\n",
    "        if (env_steps + 1) % cfg.overrides.epoch_length == 0:\n",
    "            avg_reward = eval_and_save_vid(eval_env, agent, n_eval_episodes=num_eval_episodes)\n",
    "            all_eval_rewards.append(avg_reward)\n",
    "            total_steps_eval.append(total_steps_train[-1])\n",
    "\n",
    "            eval_result = np.stack((total_steps_eval[1:], all_eval_rewards[1:]), axis=-1)\n",
    "            pd.DataFrame(eval_result).to_csv(os.path.join(work_dir, \"{}_result.csv\".format(\"eval_curve\")))\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            ax.plot(total_steps_eval[1:], all_eval_rewards[1:], 'bs-')\n",
    "            ax.set_xlabel(\"Samples\")\n",
    "            ax.set_ylabel(\"Eval reward\")\n",
    "            fig.savefig(os.path.join(work_dir, \"output_eval.png\"))        \n",
    "            plt.close(fig)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}. \"\n",
    "                f\"SAC buffer size: {len(sac_buffer)}. \"\n",
    "                f\"Real buffer size: {len(replay_buffer)}. \"\n",
    "                f\"Rollout length: {rollout_length}. \"\n",
    "                f\"Steps: {env_steps}\"\n",
    "                f\"Average reward: {avg_reward} \"\n",
    "            )\n",
    "            \n",
    "            if avg_reward > best_eval_reward:\n",
    "                best_eval_reward = avg_reward\n",
    "                agent.sac_agent.save_checkpoint(\n",
    "                    ckpt_path=os.path.join(work_dir, \"sac.pth\")\n",
    "                )\n",
    "                print(\"Saved best model\")\n",
    "            epoch += 1\n",
    "\n",
    "        env_steps += 1\n",
    "        obs = next_obs\n",
    "        trial_reward += reward\n",
    "        trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "        steps_trial += 1\n",
    "\n",
    "        # Save data for plotting training performances\n",
    "        (tcp_pos_workframe, \n",
    "        tcp_rpy_workframe,\n",
    "        cur_obj_pos_workframe, \n",
    "        cur_obj_rpy_workframe) = env.get_obs_workframe()\n",
    "        trial_push_result.append(np.hstack([trial,\n",
    "                                        steps_trial,\n",
    "                                        trial_pb_steps * env._sim_time_step,\n",
    "                                        tcp_pos_workframe, \n",
    "                                        cur_obj_pos_workframe, \n",
    "                                        tcp_rpy_workframe[2],\n",
    "                                        cur_obj_rpy_workframe[2],\n",
    "                                        env.goal_pos_workframe[0:2], \n",
    "                                        env.goal_rpy_workframe[2],\n",
    "                                        trial_reward, \n",
    "                                        info[\"tip_in_contact\"],\n",
    "                                        done]))\n",
    "        \n",
    "    # Plot losses at the end of each epoch\n",
    "    data = [train_losses, val_scores, qf1_losses, qf2_losses, policy_losses]\n",
    "    ylabels = [\"Train loss\", \"Val score\", \"QF1 loss\", \"QF2 loss\", \"Policy loss\"]\n",
    "    plot_and_save(data, ylabel=ylabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (512, 2)) of distribution Normal(loc: torch.Size([512, 2]), scale: torch.Size([512, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m (\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     qf1_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     qf2_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     policy_loss,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     _,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     _,\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m ) \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msac_agent\u001b[39m.\u001b[39;49mupdate_parameters(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     which_buffer,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     cfg\u001b[39m.\u001b[39;49moverrides\u001b[39m.\u001b[39;49msac_batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     updates_made,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     logger\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     reverse_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_push_mbpo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/third_party/pytorch_sac_pranz24/sac.py:98\u001b[0m, in \u001b[0;36mSAC.update_parameters\u001b[0;34m(self, memory, batch_size, updates, logger, reverse_mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m     mask_batch \u001b[39m=\u001b[39m mask_batch\u001b[39m.\u001b[39mlogical_not()\n\u001b[1;32m     97\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 98\u001b[0m     next_state_action, next_state_log_pi, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m     99\u001b[0m         next_state_batch\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     qf1_next_target, qf2_next_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_target(\n\u001b[1;32m    102\u001b[0m         next_state_batch, next_state_action\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m     min_qf_next_target \u001b[39m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m         torch\u001b[39m.\u001b[39mmin(qf1_next_target, qf2_next_target)\n\u001b[1;32m    106\u001b[0m         \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m next_state_log_pi\n\u001b[1;32m    107\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/third_party/pytorch_sac_pranz24/model.py:104\u001b[0m, in \u001b[0;36mGaussianPolicy.sample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    102\u001b[0m mean, log_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(state)\n\u001b[1;32m    103\u001b[0m std \u001b[39m=\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> 104\u001b[0m normal \u001b[39m=\u001b[39m Normal(mean, std)\n\u001b[1;32m    105\u001b[0m x_t \u001b[39m=\u001b[39m normal\u001b[39m.\u001b[39mrsample()  \u001b[39m# for reparameterization trick (mean + std * N(0,1))\u001b[39;00m\n\u001b[1;32m    106\u001b[0m y_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(x_t)\n",
      "File \u001b[0;32m~/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/torch/distributions/normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 50\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m constraint\u001b[39m.\u001b[39mcheck(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param))\u001b[39m.\u001b[39mall():\n\u001b[1;32m     63\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe parameter \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m has invalid values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(param))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (512, 2)) of distribution Normal(loc: torch.Size([512, 2]), scale: torch.Size([512, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "(\n",
    "    qf1_loss,\n",
    "    qf2_loss,\n",
    "    policy_loss,\n",
    "    _,\n",
    "    _,\n",
    ") = agent.sac_agent.update_parameters(\n",
    "    which_buffer,\n",
    "    cfg.overrides.sac_batch_size,\n",
    "    updates_made,\n",
    "    logger=None,\n",
    "    reverse_mask=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_gym_mbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
