{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, shutil\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "from mbrl.util.math import euler_to_quaternion, quaternion_rotation_matrix\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a display to render image\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Environment and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo_name': 'ppo', 'env_name': 'object_push-v0', 'max_ep_len': 2000, 'image_size': [128, 128], 'env_modes': {'movement_mode': 'TyRz', 'control_mode': 'TCP_position_control', 'rand_init_orn': False, 'rand_obj_mass': False, 'traj_type': 'point', 'observation_mode': 'tactile_pose_goal_excluded_data', 'reward_mode': 'dense', 'terminate_early': True, 'use_contact': True, 'task': 'goal_pos', 'additional_reward_settings': 'john_guide_off', 'mpc_goal_orn_update': False, 'planar_states': True, 'importance_obj_goal_pos': 1.0, 'importance_obj_goal_orn': 1.5, 'importance_tip_obj_orn': 1.0, 'tcp_lims': [[-0.1, 0.3], [-0.3, 0.3], [-0.0, 0.0], [-0.0, 0.0], [-0.0, 0.0], [-3.141592653589793, 3.141592653589793]], 'goal_edges': [(1, 0)], 'goal_ranges': [0.0, 0.27, -0.18, 0.18]}, 'policy': 'MultiInputPolicy', 'seed': 1, 'n_stack': 1, 'total_timesteps': 1000000, 'n_eval_episodes': 10, 'n_envs': 10, 'eval_freq': 2000.0}\n",
      "{'policy_kwargs': {'features_extractor_class': <class 'tactile_gym.sb3_helpers.custom.custom_torch_layers.CustomCombinedExtractor'>, 'features_extractor_kwargs': {'cnn_base': <class 'stable_baselines3.common.torch_layers.NatureCNN'>, 'cnn_output_dim': 256, 'mlp_extractor_net_arch': [64, 64]}, 'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>}, 'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.95, 'gae_lambda': 0.9, 'clip_range': 0.2, 'clip_range_vf': None, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'use_sde': False, 'sde_sample_freq': -1, 'target_kl': 0.1}\n",
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar  8 2021 17:26:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    }
   ],
   "source": [
    "# Make the pushing environment\n",
    "algo_name = 'ppo'\n",
    "env_name = 'object_push-v0'\n",
    "rl_params, algo_params, augmentations = import_parameters(env_name, algo_name)\n",
    "rl_params[\"env_modes\"][ 'observation_mode'] = 'tactile_pose_goal_excluded_data'\n",
    "rl_params[\"env_modes\"][ 'control_mode'] = 'TCP_position_control'\n",
    "# rl_params[\"env_modes\"]['movement_mode'] = 'TxTyRz'\n",
    "rl_params[\"max_ep_len\"] = 2000\n",
    "rl_params[\"env_modes\"][ 'terminate_early']  = True\n",
    "rl_params[\"env_modes\"][ 'use_contact'] = True\n",
    "rl_params[\"env_modes\"][ 'traj_type'] = 'point'\n",
    "rl_params[\"env_modes\"][ 'task'] = \"goal_pos\"\n",
    "rl_params[\"env_modes\"]['additional_reward_settings'] = 'john_guide_off'\n",
    "rl_params[\"env_modes\"]['mpc_goal_orn_update'] = False\n",
    "rl_params[\"env_modes\"]['planar_states'] = True\n",
    "rl_params[\"env_modes\"]['importance_obj_goal_pos'] = 1.0\n",
    "rl_params[\"env_modes\"]['importance_obj_goal_orn'] = 1.5\n",
    "rl_params[\"env_modes\"]['importance_tip_obj_orn'] = 1.0\n",
    "\n",
    "# set limits and goals\n",
    "TCP_lims = np.zeros(shape=(6, 2))\n",
    "TCP_lims[0, 0], TCP_lims[0, 1] = -0.1, 0.3  # x lims\n",
    "TCP_lims[1, 0], TCP_lims[1, 1] = -0.3, 0.3  # y lims\n",
    "TCP_lims[2, 0], TCP_lims[2, 1] = -0.0, 0.0  # z lims\n",
    "TCP_lims[3, 0], TCP_lims[3, 1] = -0.0, 0.0  # roll lims\n",
    "TCP_lims[4, 0], TCP_lims[4, 1] = -0.0, 0.0  # pitch lims\n",
    "TCP_lims[5, 0], TCP_lims[5, 1] = -180 * np.pi / 180, 180 * np.pi / 180  # yaw lims\n",
    "\n",
    "# goal parameter\n",
    "# goal_edges = [(0, -1), (0, 1), (1, 0)] # Top bottom and stright\n",
    "goal_edges = [(1, 0)]\n",
    "goal_x_max = np.float64(TCP_lims[0, 1] * 0.9).item()\n",
    "goal_x_min = 0.0 # np.float64(TCP_lims[0, 0] * 0.6).item()\n",
    "goal_y_max = np.float64(TCP_lims[1, 1] * 0.6).item()\n",
    "goal_y_min = np.float64(TCP_lims[1, 0] * 0.6).item()\n",
    "goal_ranges = [goal_x_min, goal_x_max, goal_y_min, goal_y_max]\n",
    "\n",
    "rl_params[\"env_modes\"]['tcp_lims'] = TCP_lims.tolist()\n",
    "rl_params[\"env_modes\"]['goal_edges'] = goal_edges\n",
    "rl_params[\"env_modes\"]['goal_ranges'] = goal_ranges\n",
    "\n",
    "print(rl_params)\n",
    "print(algo_params)\n",
    "\n",
    "env_kwargs={\n",
    "    'show_gui':False,\n",
    "    'show_tactile':False,\n",
    "    'max_steps':rl_params[\"max_ep_len\"],\n",
    "    'image_size':rl_params[\"image_size\"],\n",
    "    'env_modes':rl_params[\"env_modes\"],\n",
    "}\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# This functions allows the model to evaluate the true rewards given an observation \n",
    "reward_fn = reward_fns.cartpole\n",
    "# This function allows the model to know if an observation should make the episode end\n",
    "term_fn = termination_fns.cartpole\n",
    "\n",
    "# Define model working directorys\n",
    "work_dir = os.path.join(os.getcwd(), 'saved_model')\n",
    "# work_dir = os.path.join(os.getcwd(), 'training_cfg')\n",
    "if not os.path.exists(work_dir):\n",
    "    os.mkdir(work_dir)\n",
    "else:\n",
    "    for filename in os.listdir(work_dir):\n",
    "        file_path = os.path.join(work_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000937 0.000079 0.000006 1.000000 -0.001000 0.000000 0.000000 1.000000]\n",
      "[0.000937 0.000079 0.000006 1.000000 -0.001000 0.000000 0.000000 1.000000]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())\n",
    "print(env.get_observation())\n",
    "print(type(env.get_observation()))\n",
    "# print(env.get_tactile_pose_obs_goal_exluded())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8,)\n",
      "Box(2,)\n",
      "(8,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(obs_shape)\n",
    "print(act_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = env._max_steps\n",
    "num_trials = 1\n",
    "ensemble_size = 5\n",
    "initial_buffer_size = 2000\n",
    "buffer_size = num_trials * trial_length\n",
    "target_normalised = True\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "        \"target_normalize\": target_normalised,\n",
    "        \"dataset_size\": buffer_size\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneDTransitionRewardModel(\n",
      "  (model): GaussianMLP(\n",
      "    (hidden_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=10, out_size=200, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=200, out_size=200, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=200, out_size=200, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (mean_and_logvar): EnsembleLinearLayer(num_members=5, in_size=200, out_size=16, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "\n",
    "print(dynamics_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_type = \"cem\"\n",
    "\n",
    "if optimizer_type == \"cem\":\n",
    "    optimizer_cfg = {\n",
    "            \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "            \"num_iterations\": 5,\n",
    "            \"elite_ratio\": 0.1,\n",
    "            \"population_size\": 500,\n",
    "            \"alpha\": 0.1,\n",
    "            \"device\": device,\n",
    "            \"lower_bound\": \"???\",\n",
    "            \"upper_bound\": \"???\",\n",
    "            \"return_mean_elites\": True,\n",
    "            \"clipped_normal\": False\n",
    "        }\n",
    "elif optimizer_type == \"mppi\":\n",
    "    optimizer_cfg = {\n",
    "            \"_target_\": \"mbrl.planning.MPPIOptimizer\",\n",
    "            \"num_iterations\": 5,\n",
    "            \"gamma\": 1.0,\n",
    "            \"population_size\": 500,\n",
    "            \"sigma\": 0.95,\n",
    "            \"beta\": 0.7,\n",
    "            \"lower_bound\": \"???\",\n",
    "            \"upper_bound\": \"???\",\n",
    "            \"device\": device,\n",
    "        }\n",
    "\n",
    "elif optimizer_type == \"icem\":\n",
    "    optimizer_cfg = {\n",
    "            \"_target_\": \"mbrl.planning.ICEMOptimizer\",\n",
    "            \"num_iterations\": 5,\n",
    "            \"elite_ratio\": 0.1,\n",
    "            \"population_size\": 500,\n",
    "            \"population_decay_factor\": 1.25,\n",
    "            \"colored_noise_exponent\": 2.0,\n",
    "            \"keep_elite_frac\": 0.1,\n",
    "            \"alpha\": 0.1,\n",
    "            \"lower_bound\": \"???\",\n",
    "            \"upper_bound\": \"???\",\n",
    "            \"device\": device,\n",
    "        }\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 40,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": optimizer_cfg\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving config files\n",
    "config_filename = 'cfg_dict'\n",
    "config_dir = os.path.join(work_dir, config_filename)\n",
    "omegaconf.OmegaConf.save(config=cfg, f=config_dir) \n",
    "loaded = omegaconf.OmegaConf.load(config_dir)\n",
    "assert cfg == loaded\n",
    "\n",
    "agent_config_filename = 'agent_cfg'\n",
    "agent_config_dir = os.path.join(work_dir, agent_config_filename)\n",
    "omegaconf.OmegaConf.save(config=agent_cfg, f=agent_config_dir) \n",
    "loaded = omegaconf.OmegaConf.load(agent_config_dir)\n",
    "assert agent_cfg == loaded\n",
    "\n",
    "env_kwargs_filename = 'env_kwargs'\n",
    "env_kwargs_dir = os.path.join(work_dir, env_kwargs_filename)\n",
    "omegaconf.OmegaConf.save(config=env_kwargs, f=env_kwargs_dir) \n",
    "loaded = omegaconf.OmegaConf.load(env_kwargs_dir)\n",
    "assert env_kwargs == loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current goal index 0\n",
      "Position goal trajectory [[0.270000 0.149340 0.000000]]\n",
      "0\n",
      "[[ True  True  True]]\n",
      "[0.270000 0.149340 0.000000]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Current goal index\", env.targ_traj_list_id)\n",
    "print(\"Position goal trajectory\", env.traj_pos_workframe)\n",
    "# print(\"Orientation goal trajectory\", env.traj_rpy_workframe)\n",
    "# print(\"Orientation goal trajectory\", env.traj_orn_workframe)\n",
    "\n",
    "# All of this can be accessed through model_env.env\n",
    "print(model_env.env.targ_traj_list_id)\n",
    "print(model_env.env.traj_pos_workframe == env.traj_pos_workframe)\n",
    "print(model_env.env.goal_pos_workframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True],\n",
      "        [False],\n",
      "        [False]], device='cuda:0')\n",
      "tensor([[ True],\n",
      "        [False],\n",
      "        [False]], device='cuda:0')\n",
      "tensor([[-100.],\n",
      "        [   0.],\n",
      "        [   0.]], device='cuda:0')\n",
      "tensor([[-100.],\n",
      "        [   0.],\n",
      "        [   0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Test termination function in model env -------------------------\n",
    "early_termination = env.terminate_early\n",
    "def termination(act: torch.Tensor, next_obs: torch.Tensor, rewards:  torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Criteria for terminating an episode. Should return a vector of dones of size \n",
    "    population_size x batch_size\n",
    "    '''\n",
    "\n",
    "    if 'reduced' in env.observation_mode: \n",
    "        tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "        # tcp_orn_to_goal_workframe = next_obs[:, 3:7]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 7:10]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 10:13]\n",
    "        cur_obj_pos_to_goal_workframe = next_obs[:, 13:16]\n",
    "        # cur_obj_orn_to_goal_workframe = next_obs[:, 16:20]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 20:23]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 23:26]\n",
    "\n",
    "        tcp_pos_workframe = tcp_pos_to_goal_workframe + goal_pos_workframe_batch\n",
    "        cur_obj_pos_workframe = cur_obj_pos_to_goal_workframe + goal_pos_workframe_batch\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_to_goal_workframe, axis=1)\n",
    "    \n",
    "    elif env.observation_mode == 'tactile_pose_data': \n",
    "\n",
    "        if env.planar_states == True: \n",
    "            # tcp_pos_to_goal_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            tcp_orn_to_goal_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "            cur_obj_pos_to_goal_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            cur_obj_orn_to_goal_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "\n",
    "            # tcp_pos_to_goal_workframe[:, 0:2] = next_obs[:, 0:2]\n",
    "            tcp_orn_to_goal_workframe[:, 2:4] = next_obs[:, 0:2]\n",
    "            cur_obj_pos_to_goal_workframe[:, 0:2]= next_obs[:, 2:4]\n",
    "            cur_obj_orn_to_goal_workframe[:, 2:4] = next_obs[:, 4:6]\n",
    "        else:\n",
    "            # tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "            tcp_orn_to_goal_workframe = next_obs[:, 0:4]\n",
    "            cur_obj_pos_to_goal_workframe = next_obs[:, 4:7]\n",
    "            cur_obj_orn_to_goal_workframe = next_obs[:, 7:11]\n",
    "\n",
    "        # Only take in the x and y coordinates\n",
    "        # tcp_pos_workframe = tcp_pos_to_goal_workframe[:, 0:2] + goal_pos_workframe_batch[:, 0:2]\n",
    "        cur_obj_pos_workframe = cur_obj_pos_to_goal_workframe[:, 0:2] + goal_pos_workframe_batch[:, 0:2]\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_to_goal_workframe, axis=1)\n",
    "            \n",
    "    elif env.observation_mode == 'tactile_pose_goal_excluded_data': \n",
    "\n",
    "        if env.planar_states == True:\n",
    "            tcp_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float64).to(device)\n",
    "            tcp_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float64).to(device)\n",
    "            cur_obj_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float64).to(device)\n",
    "            cur_obj_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float64).to(device)\n",
    "\n",
    "            tcp_pos_workframe[:, 0:2] = next_obs[:, 0:2]\n",
    "            tcp_orn_workframe[:, 2:4] = next_obs[:, 2:4]\n",
    "            cur_obj_pos_workframe[:, 0:2]= next_obs[:, 4:6]\n",
    "            cur_obj_orn_workframe[:, 2:4] = next_obs[:, 6:8]\n",
    "        else:   \n",
    "\n",
    "            tcp_pos_workframe = next_obs[:, 0:3]\n",
    "            tcp_orn_workframe = next_obs[:, 3:7]\n",
    "            cur_obj_pos_workframe = next_obs[:, 7:10]\n",
    "            cur_obj_orn_workframe = next_obs[:, 10:14]\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_workframe - goal_pos_workframe_batch, axis=1)\n",
    "\n",
    "        # calculate tcp to object orn\n",
    "        tcp_to_obj_orn = model_env.get_orn_dist(cur_obj_orn_workframe, tcp_orn_workframe)\n",
    "\n",
    "        tip_to_obj_pos = torch.linalg.norm(tcp_pos_workframe - cur_obj_pos_workframe, axis=1)\n",
    "\n",
    "    else:\n",
    "        tcp_pos_workframe = next_obs[:, 0:3]\n",
    "        # tcp_rpy_workframe = next_obs[:, 3:6]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 6:9]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 9:12]\n",
    "        cur_obj_pos_workframe = next_obs[:, 12:15]\n",
    "        # cur_obj_rpy_workframe = next_obs[:, 15:18]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 18:21]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 21:24]\n",
    "        # pred_goal_pos_workframe = next_obs[:, 24:27]\n",
    "        # pred_goal_rpy_workframe = next_obs[:, 27:30]\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_workframe - goal_pos_workframe_batch, axis=1)\n",
    "    \n",
    "    # print('Object position, ', cur_obj_pos_workframe)\n",
    "    # print('TCP position, ', tcp_pos_workframe)\n",
    "    # print(obj_goal_pos_dist)\n",
    "\n",
    "    # Set obj to goal to smaller than tolerance for testing\n",
    "    # obj_goal_pos_dist[1] = 0.001\n",
    "\n",
    "    # intiailise terminated vector\n",
    "    terminated = torch.zeros((batch_size, 1), dtype=bool).to(device)\n",
    "\n",
    "    # print(\"goal position batch before update \\n\", goal_pos_workframe_batch)\n",
    "    # print(\"goal index batch before update\", targ_traj_list_id_batch)\n",
    "\n",
    "    # Early termination if outside of the tcp limits\n",
    "    if early_termination:\n",
    "        outside_tcp_lims_idx = outside_tcp_lims(cur_obj_pos_workframe, tcp_to_obj_orn, tip_to_obj_pos)\n",
    "        terminated[outside_tcp_lims_idx] = True\n",
    "        rewards[outside_tcp_lims_idx] += env.terminated_early_penalty\n",
    "        # print(\"Outside TPC_lims, \", outside_tcp_lims(tcp_pos_workframe, cur_obj_pos_workframe, tip_to_obj_pos))\n",
    "\n",
    "    # update goals index if subgoal reached\n",
    "    targ_traj_list_id_batch[obj_goal_pos_dist < model_env.termination_pos_dist] += 1\n",
    "\n",
    "    # Terminated is true if last subgoal is reached\n",
    "    terminated[targ_traj_list_id_batch >= model_env.traj_n_points] = True\n",
    "    rewards[targ_traj_list_id_batch >= model_env.traj_n_points] += model_env.reached_goal_reward\n",
    "\n",
    "    # Update goal position batch for none terminated samples\n",
    "    goal_pos_workframe_batch[~terminated[:,0]] = traj_pos_workframe[targ_traj_list_id_batch[~terminated[:,0]]]\n",
    "\n",
    "    # print(\"obj to goal distance\", obj_goal_pos_dist)\n",
    "    # print(\"goal index batch\", targ_traj_list_id_batch)\n",
    "    # print(\"terminated batch\", terminated)\n",
    "    # print(\"goal position index not terminated\", targ_traj_list_id_batch[~terminated[:,0]])\n",
    "    # print(\"The none terminated goals to be updated\", traj_pos_workframe[targ_traj_list_id_batch[~terminated[:,0]]])\n",
    "    # print(\"The updated goals \\n\", goal_pos_workframe_batch)\n",
    "    return terminated\n",
    "\n",
    "def outside_tcp_lims(cur_obj_pos_workframe, tcp_to_obj_orn, tip_to_obj_pos):\n",
    "    # xyz_tcp_dist_to_obj = torch.linalg.norm(tcp_pos_workframe - cur_obj_pos_workframe)\n",
    "    return ((cur_obj_pos_workframe[:, 0] < env.robot.arm.TCP_lims[0,0]) | \n",
    "        (cur_obj_pos_workframe[:, 0] > env.robot.arm.TCP_lims[0,1]) | \n",
    "        (cur_obj_pos_workframe[:, 1] < env.robot.arm.TCP_lims[1,0]) | \n",
    "        (cur_obj_pos_workframe[:, 1] > env.robot.arm.TCP_lims[1,1]) | \n",
    "        (tcp_to_obj_orn > model_env.max_tcp_to_obj_orn) | \n",
    "        (tip_to_obj_pos > model_env.max_tip_to_obj_pos))\n",
    "        # (xyz_tcp_dist_to_obj > env.obj_width / 2))                # TODO: exiting episode when roughly lose contact\n",
    "\n",
    "\n",
    "# Reset environment\n",
    "batch_size = 3\n",
    "env.reset()\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# Create goal batches (access through model_env)\n",
    "traj_pos_workframe = model_env.traj_pos_workframe.clone()\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch.clone()\n",
    "targ_traj_list_id_batch = model_env.targ_traj_list_id_batch\n",
    "# targ_traj_list_id_batch = torch.from_numpy(targ_traj_list_id_batch).long()\n",
    "# targ_traj_list_id_batch[0] = 11\n",
    "\n",
    "tcp_pos = torch.tensor([[0.1, 0.01, 0], [0.1, 0.01, 0], [0.1, 0.01, 0]]).to(device)\n",
    "obj_pos = tcp_pos + torch.tensor([[0.0, 0.1, 0.0], [0.0, 0, 0], [0, 0, 0]]).to(device)\n",
    "tcp_orn = euler_to_quaternion(torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]])).to(device)\n",
    "obj_orn = euler_to_quaternion(torch.tensor([[0, 0, 10/180*np.pi], [0, 0, 10/180*np.pi], [0, 0, 10/180*np.pi]])).to(device)\n",
    "tip_to_obj_orn = model_env.get_orn_dist(tcp_orn, obj_orn)\n",
    "\n",
    "obs = torch.randn(batch_size, obs_shape[0]).to(device)\n",
    "if \"reduced\" in env.observation_mode:\n",
    "    obs[:, :3] =  torch.tensor([[0.2, 0.05, 0.1], [0.2, 0.05, 0.1], [0.2, 0.05, 0.1]]) \n",
    "    obs[:, 13:16] = torch.tensor([[0.2, 0.05, 0.1], [0.2, 0.1, 0.1], [0.2, 0.05, 0.1]])\n",
    "elif env.observation_mode =='tactile_pose_data':\n",
    "    if env.planar_states == True:\n",
    "        # obs[:, 0:2] =  torch.tensor([[0.20, 0.01], [0.3, 0.01], [0.2, 0.01]]).to(device) - goal_pos_workframe_batch[:, 0:2]\n",
    "        obs[:, 2:4] = torch.tensor([[0.4, 0.01], [0.35, 0.1], [0.2, 0.01]]).to(device) - goal_pos_workframe_batch[:, 0:2]\n",
    "    else:\n",
    "        # obs[:, 0:2] =  torch.tensor([[0.20, 0.01], [0.3, 0.01], [0.2, 0.01]]).to(device) - goal_pos_workframe_batch[:, 0:2]\n",
    "        obs[:, 4:6] = torch.tensor([[0.30, 0.01], [0.20, 0.09], [0.2, 0.01]]).to(device) - goal_pos_workframe_batch[:, 0:2]\n",
    "elif env.observation_mode == 'tactile_pose_goal_excluded_data': \n",
    "    if env.planar_states == True:\n",
    "        obs[:, 0:2] = tcp_pos[:, 0:2]\n",
    "        obs[:, 4:6] = obj_pos[:, 0:2]\n",
    "        obs[:, 2:4] = tcp_orn[:, 2:4]\n",
    "        obs[:, 6:8] = obj_orn[:, 2:4]\n",
    "    else:\n",
    "        # obs[:, 0:2] =  torch.tensor([[0.2, 0.01], [0.31, 0.01], [0.2, 0.01]]) \n",
    "        obs[:, 4:6] = torch.tensor([[0.31, 0.1], [0.20, 0.01], [0.2, 0.01]])\n",
    "\n",
    "else:\n",
    "    obs[:, :3] =  torch.tensor([[0.3, 0.1, 0.1], [0.3, 0.1, 0.1], [0.3, 0.1, 0.1]]) \n",
    "    obs[:, 12:15] = torch.tensor([[0.3, 0.1, 0.1], [0.3, 0.1, 0.1], [0.4, 0.1, 0.1]]) \n",
    "\n",
    "act = torch.randn(batch_size, 1).to(device)\n",
    "rewards_test = torch.zeros(batch_size, 1).to(device)\n",
    "rewards_push = torch.zeros(batch_size, 1).to(device)\n",
    "print(termination(act, obs, rewards_test))\n",
    "print(model_env.termination(act, obs, rewards_push))\n",
    "print(rewards_test)\n",
    "print(rewards_push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False], device='cuda:0')\n",
      "tensor([[ True],\n",
      "        [False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Test early termination ------------------------\n",
    "early_termination = True\n",
    "\n",
    "tcp_pos_workframe = torch.tensor([[0.1, 0.01, 0], [0.1, 0.01, 0]]).to(device)\n",
    "cur_obj_pos_workframe = tcp_pos_workframe + torch.tensor([[0.1, 0.0, 0.0], [0.0, 0, 0]]).to(device) # The first sample for obj_pos is out of y_lims\n",
    "tcp_orn = euler_to_quaternion(torch.tensor([[0, 0, 10/180*np.pi], [0, 0, 10/180*np.pi]])).to(device)\n",
    "obj_orn = euler_to_quaternion(torch.tensor([[0, 0, 10/180*np.pi], [0, 0, 30/180*np.pi]])).to(device)\n",
    "tip_to_obj_orn = model_env.get_orn_dist(tcp_orn, obj_orn)\n",
    "tip_to_obj_pos = torch.linalg.norm(tcp_pos_workframe - cur_obj_pos_workframe,  axis=1)\n",
    "print(outside_tcp_lims(cur_obj_pos_workframe, tip_to_obj_orn, tip_to_obj_pos))\n",
    "\n",
    "batch_size = 2\n",
    "env.reset()\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# Create goal batches (access through model_env)\n",
    "traj_pos_workframe = model_env.traj_pos_workframe\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch\n",
    "targ_traj_list_id_batch = model_env.targ_traj_list_id_batch\n",
    "\n",
    "obs = torch.randn(batch_size, 30).to(device)\n",
    "act = torch.randn(batch_size, 1).to(device)\n",
    "rewards = torch.randn(batch_size).to(device)\n",
    "if \"reduced\" in env.observation_mode:\n",
    "    obs[:, 0:3] = tcp_pos_workframe\n",
    "    obs[:, 13:16] = cur_obj_pos_workframe\n",
    "elif  env.observation_mode == 'tactile_pose_data':\n",
    "    if env.planar_states == True:\n",
    "        # obs[:, 0:2] = tcp_pos_workframe[:, 0:2] - goal_pos_workframe_batch[:, 0:2]\n",
    "        obs[:, 2:4] = cur_obj_pos_workframe[:, 0:2] - goal_pos_workframe_batch[:, 0:2]\n",
    "    else:\n",
    "        # obs[:, 0:3] = tcp_pos_workframe - goal_pos_workframe_batch\n",
    "        obs[:, 4:7] = cur_obj_pos_workframe- goal_pos_workframe_batch  \n",
    "elif env.observation_mode == 'tactile_pose_goal_excluded_data':\n",
    "        if env.planar_states == True:\n",
    "            obs[:, 0:2] = tcp_pos_workframe[:, 0:2]\n",
    "            obs[:, 2:4] = tcp_orn[:, 2:4]\n",
    "            obs[:, 4:6] = cur_obj_pos_workframe[:, 0:2]\n",
    "            obs[:, 6:8] = obj_orn[:, 2:4]\n",
    "        else:\n",
    "            # obs[:, 0:3] = tcp_pos_workframe\n",
    "            obs[:, 4:7] = cur_obj_pos_workframe\n",
    "else:\n",
    "    obs[:, 0:3] = tcp_pos_workframe\n",
    "    obs[:, 12:15] = cur_obj_pos_workframe\n",
    "print(termination(_, obs, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0771899710385694\n",
      "tensor([[-0.0772],\n",
      "        [-0.0772]], device='cuda:0')\n",
      "tensor([[-0.0772],\n",
      "        [-0.0772]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# --------------- Test reward function  in model env-------------------------------\n",
    "def xyz_obj_dist_to_goal(cur_obj_pos_workframe):\n",
    "\n",
    "    # obj to goal distance\n",
    "    return torch.linalg.norm(cur_obj_pos_workframe - goal_pos_workframe_batch, axis=1)\n",
    "\n",
    "def get_pos_dist(pos_dist_vector):\n",
    "\n",
    "    # obj to goal distance\n",
    "    return torch.linalg.norm(pos_dist_vector, axis=1)\n",
    "\n",
    "def orn_obj_dist_to_goal_rpy(cur_obj_rpy_workframe):\n",
    "    cur_obj_orn_workframe = euler_to_quaternion(cur_obj_rpy_workframe)\n",
    "    inner_product = torch.sum(goal_orn_workframe_batch*cur_obj_orn_workframe, 1)\n",
    "    return torch.arccos(torch.clip(2 * (inner_product ** 2) - 1, -1, 1))\n",
    "\n",
    "def orn_obj_dist_to_goal_orn(cur_obj_orn_workframe):\n",
    "    inner_product = torch.sum(goal_orn_workframe_batch*cur_obj_orn_workframe, 1)\n",
    "    return torch.arccos(torch.clip(2 * (inner_product ** 2) - 1, -1, 1))\n",
    "\n",
    "def get_orn_norm(orn_dist_vector):\n",
    "    \"\"\"\n",
    "    Distance between the current obj orientation and goal orientation.\n",
    "    \"\"\"\n",
    "    dist = torch.arccos(torch.clip(\n",
    "        (2 * (orn_dist_vector[:, 3]**2)) - 1, -1, 1))\n",
    "    return dist\n",
    "\n",
    "def get_orn_dist(orn_dist_vector_1, orn_dist_vector_2):\n",
    "    inner_product = torch.sum(orn_dist_vector_1*orn_dist_vector_2, 1)\n",
    "    return torch.arccos(torch.clip(2 * (inner_product ** 2) - 1, -1, 1))\n",
    "\n",
    "def cos_tcp_dist_to_obj(cur_obj_rpy_workframe, tcp_rpy_workframe):\n",
    "    \"\"\"\n",
    "    Cos distance from current orientation of the TCP to the current\n",
    "    orientation of the object\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = cur_obj_rpy_workframe.shape[0]\n",
    "\n",
    "    # tip normal to object normal\n",
    "    cur_obj_orn_workframe = euler_to_quaternion(cur_obj_rpy_workframe)\n",
    "    obj_rot_matrix_workframe = quaternion_rotation_matrix(cur_obj_orn_workframe)\n",
    "    obj_rot_matrix_workframe = torch.reshape(obj_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    obj_init_vector_workframe = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32).to(device)\n",
    "    obj_vector_workframe = torch.matmul(obj_rot_matrix_workframe, obj_init_vector_workframe)\n",
    "    # obj_vector_workframe = obj_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    tcp_orn_workframe = euler_to_quaternion(tcp_rpy_workframe)\n",
    "    tip_rot_matrix_workframe = quaternion_rotation_matrix(tcp_orn_workframe)\n",
    "    tip_rot_matrix_workframe  = torch.reshape(tip_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    tip_init_vector_workframe  = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32).to(device)\n",
    "    tip_vector_workframe  = torch.matmul(tip_rot_matrix_workframe, tip_init_vector_workframe)\n",
    "    # tip_vector_workframe = tip_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    obj_tip_dot_product = torch.sum(obj_vector_workframe*tip_vector_workframe, 1)\n",
    "    cos_sim_workfrfame = obj_tip_dot_product / (\n",
    "        torch.linalg.norm(obj_vector_workframe, axis=1) * torch.linalg.norm(tip_vector_workframe, axis=1)\n",
    "    )\n",
    "    cos_dist_workframe = 1 - cos_sim_workfrfame\n",
    "\n",
    "    return cos_dist_workframe\n",
    "\n",
    "def cos_tcp_dist_to_obj_reduced(cur_obj_orn_to_goal_workframe, tcp_orn_to_goal_workframe):\n",
    "    \"\"\"\n",
    "    Cos distance from current orientation of the TCP to the current\n",
    "    orientation of the object\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = cur_obj_orn_to_goal_workframe.shape[0]\n",
    "\n",
    "    # tip normal to object normal\n",
    "    obj_rot_matrix_workframe = quaternion_rotation_matrix(cur_obj_orn_to_goal_workframe)\n",
    "    obj_rot_matrix_workframe = torch.reshape(obj_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    # obj_init_vector_workframe = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)\n",
    "    # obj_vector_workframe = torch.matmul(obj_rot_matrix_workframe, obj_init_vector_workframe)\n",
    "    obj_vector_workframe = obj_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    tip_rot_matrix_workframe = quaternion_rotation_matrix(tcp_orn_to_goal_workframe)\n",
    "    tip_rot_matrix_workframe  = torch.reshape(tip_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    # tip_init_vector_workframe  = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)\n",
    "    # tip_vector_workframe  = torch.matmul(tip_rot_matrix_workframe, tip_init_vector_workframe)\n",
    "    tip_vector_workframe = tip_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    obj_tip_dot_product = torch.sum(obj_vector_workframe*tip_vector_workframe, 1)\n",
    "    cos_sim_workfrfame = obj_tip_dot_product / (\n",
    "        torch.linalg.norm(obj_vector_workframe, axis=1) * torch.linalg.norm(tip_vector_workframe, axis=1)\n",
    "    )\n",
    "    cos_dist_workframe = 1 - cos_sim_workfrfame \n",
    "\n",
    "    return cos_dist_workframe\n",
    "\n",
    "def cos_tcp_Rz_dist_to_obj(cos_cur_obj_Rz_to_goal_workframe, cos_tcp_Rz_to_goal_workframe):\n",
    "    cos_sim_workframe = torch.cos(\n",
    "        torch.arccos(cos_tcp_Rz_to_goal_workframe) - torch.arccos(cos_cur_obj_Rz_to_goal_workframe)\n",
    "        )\n",
    "    return 1 - cos_sim_workframe\n",
    "\n",
    "def reward(act: torch.Tensor, next_obs: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Caculate the reward given a batch of observations \n",
    "    '''\n",
    "\n",
    "    batch_size = next_obs.shape[0]\n",
    "\n",
    "    if 'reduced' in env.observation_mode: \n",
    "        # tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "        tcp_orn_to_goal_workframe = next_obs[:, 3:7]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 7:10]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 10:13]\n",
    "        cur_obj_pos_to_goal_workframe = next_obs[:, 13:16]\n",
    "        cur_obj_orn_to_goal_workframe = next_obs[:, 16:20]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 20:23]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 23:26]\n",
    "\n",
    "        obj_goal_pos_dist = get_pos_dist(cur_obj_pos_to_goal_workframe)\n",
    "        obj_goal_orn_dist = get_orn_norm(cur_obj_orn_to_goal_workframe)\n",
    "        tip_obj_orn_dist = cos_tcp_dist_to_obj_reduced(cur_obj_orn_to_goal_workframe, tcp_orn_to_goal_workframe)\n",
    "\n",
    "        print(obj_goal_orn_dist)\n",
    "    elif env.observation_mode == 'tactile_pose_data': \n",
    "        if env.planar_states == True: \n",
    "            # tcp_pos_to_goal_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            tcp_orn_to_goal_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "            cur_obj_pos_to_goal_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            cur_obj_orn_to_goal_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "\n",
    "            # tcp_pos_to_goal_workframe[:, 0:2] = next_obs[:, 0:2]\n",
    "            tcp_orn_to_goal_workframe[:, 2:4] = next_obs[:, 0:2]\n",
    "            cur_obj_pos_to_goal_workframe[:, 0:2]= next_obs[:, 2:4]\n",
    "            cur_obj_orn_to_goal_workframe[:, 2:4] = next_obs[:, 4:6]\n",
    "        else:\n",
    "            # tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "            tcp_orn_to_goal_workframe = next_obs[:, 0:4]\n",
    "            cur_obj_pos_to_goal_workframe = next_obs[:, 4:7]\n",
    "            cur_obj_orn_to_goal_workframe = next_obs[:, 7:11]\n",
    "            \n",
    "        obj_goal_pos_dist = get_pos_dist(cur_obj_pos_to_goal_workframe)\n",
    "        obj_goal_orn_dist = get_orn_norm(cur_obj_orn_to_goal_workframe)\n",
    "        tip_obj_orn_dist = get_orn_dist(cur_obj_orn_to_goal_workframe, tcp_orn_to_goal_workframe)\n",
    "\n",
    "    elif env.observation_mode == 'tactile_pose_goal_excluded_data':\n",
    "        if env.planar_states == True:\n",
    "            # tcp_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            tcp_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "            cur_obj_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            cur_obj_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "\n",
    "            # tcp_pos_workframe[:, 0:2] = next_obs[:, 0:2]\n",
    "            tcp_orn_workframe[:, 2:4] = next_obs[:, 2:4]\n",
    "            cur_obj_pos_workframe[:, 0:2]= next_obs[:, 4:6]\n",
    "            cur_obj_orn_workframe[:, 2:4] = next_obs[:, 6:8]\n",
    "        else:   \n",
    "            # tcp_pos_workframe = next_obs[:, 0:3]\n",
    "            tcp_orn_workframe = next_obs[:, 0:4]\n",
    "            cur_obj_pos_workframe = next_obs[:, 4:7]\n",
    "            cur_obj_orn_workframe = next_obs[:, 7:11]\n",
    "\n",
    "        obj_goal_pos_dist = xyz_obj_dist_to_goal(cur_obj_pos_workframe)\n",
    "        obj_goal_orn_dist = orn_obj_dist_to_goal_orn(cur_obj_orn_workframe)\n",
    "        tip_obj_orn_dist = get_orn_dist(cur_obj_orn_workframe, tcp_orn_workframe)\n",
    "        \n",
    "    else:\n",
    "        # tcp_pos_workframe = next_obs[:, 0:3]\n",
    "        tcp_rpy_workframe = next_obs[:, 3:6]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 6:9]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 9:12]\n",
    "        cur_obj_pos_workframe = next_obs[:, 12:15]\n",
    "        cur_obj_rpy_workframe = next_obs[:, 15:18]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 18:21]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 21:24]\n",
    "        # pred_goal_pos_workframe = next_obs[:, 24:27]\n",
    "        # pred_goal_rpy_workframe = next_obs[:, 27:30]\n",
    "\n",
    "        obj_goal_pos_dist = xyz_obj_dist_to_goal(cur_obj_pos_workframe)\n",
    "        obj_goal_orn_dist = orn_obj_dist_to_goal_rpy(cur_obj_rpy_workframe)\n",
    "        tip_obj_orn_dist = cos_tcp_dist_to_obj(cur_obj_rpy_workframe, tcp_rpy_workframe)\n",
    "\n",
    "    reward = -(\n",
    "        (env.W_obj_goal_pos * obj_goal_pos_dist)\n",
    "        + (env.W_obj_goal_orn * obj_goal_orn_dist)\n",
    "        + (env.W_tip_obj_orn * tip_obj_orn_dist)\n",
    "        )\n",
    "    reward = reward[:, None]\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Create observation and goal batch \n",
    "batch_size = 2\n",
    "obs = env.reset()\n",
    "for i in range(1):\n",
    "    obs, _, done, _ = env.step(env.action_space.sample())\n",
    "obs = torch.tensor(obs).to(torch.float32).to(device)\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "obs_batch = torch.tile(obs, (batch_size,) + tuple([1] * obs.ndim))\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch\n",
    "goal_orn_workframe_batch = model_env.goal_orn_workframe_batch\n",
    "\n",
    "print(env.dense_reward())\n",
    "print(reward(_, obs_batch))\n",
    "print(model_env.reward(_, obs_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.000001 0.000001 0.104460 0.994529]\n",
      "tensor([[0.0000, 0.0000, 0.1045, 0.9945],\n",
      "        [0.0000, 0.0000, 0.1045, 0.9945]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# check update goal function\n",
    "batch_size = 2\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    obs, _, done, _ = env.step(env.action_space.sample())\n",
    "obs = torch.tensor(obs).to(torch.float32).to(device)\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "obs_batch = torch.tile(obs, (batch_size,) + tuple([1] * obs.ndim))\n",
    "\n",
    "print(env.goal_orn_workframe)\n",
    "model_env.update_goal_orn(obs_batch)\n",
    "print(model_env.goal_orn_workframe_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.7497],\n",
      "        [-1.2274],\n",
      "        [-1.4438],\n",
      "        ...,\n",
      "        [-0.0000],\n",
      "        [-6.4915],\n",
      "        [-0.0000]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[-6.7497],\n",
      "        [-1.2274],\n",
      "        [-1.4438],\n",
      "        ...,\n",
      "        [-0.0000],\n",
      "        [-6.4915],\n",
      "        [-0.0000]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test reward and terminal function using next_observ from model.sample()\n",
    "planning_horizon = agent.optimizer.horizon\n",
    "# initialise action sequence\n",
    "action_lb = env.action_space.low.tolist()\n",
    "action_ub = env.action_space.high.tolist()\n",
    "initial_solution = (((torch.tensor(action_lb) + torch.tensor(action_ub)) / 2)\n",
    "            .float().to(device)\n",
    "        )\n",
    "initial_solution = initial_solution.repeat((planning_horizon, 1))\n",
    "mu, dispersion = agent.optimizer.optimizer._init_population_params(initial_solution)\n",
    "action_sequences = torch.zeros((500,) + initial_solution.shape).to(device)\n",
    "action_sequences = agent.optimizer.optimizer._sample_population(mu, dispersion, action_sequences)\n",
    "\n",
    "# Intialise state and create model state for model input\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    obs, _, done, _ = env.step(env.action_space.sample())\n",
    "initial_state = obs\n",
    "tiling_shape = (20 * 500,) + tuple(\n",
    "    [1] * initial_state.ndim\n",
    ")\n",
    "initial_obs_batch = np.tile(initial_state, tiling_shape).astype(np.float32)\n",
    "model_state = model_env.reset(initial_obs_batch, return_as_np=False)\n",
    "\n",
    "# get action for time step\n",
    "action_for_step = action_sequences[:, 0, :]\n",
    "action_batch = torch.repeat_interleave(\n",
    "                    action_for_step, 20, dim=0\n",
    "                )\n",
    "\n",
    "# Get next observation from model\n",
    "(\n",
    "    next_observs,\n",
    "    pred_rewards,\n",
    "    pred_terminals,\n",
    "    next_model_state,\n",
    ") = model_env.dynamics_model.sample(\n",
    "    action_batch,\n",
    "    model_state,\n",
    "    deterministic=False,\n",
    "    rng=model_env._rng,\n",
    ")\n",
    "\n",
    "# Next obervation types\n",
    "# print(next_observs.type())\n",
    "# print(next_observs.dtype)\n",
    "# print(next_observs.shape)\n",
    "\n",
    "# Create observation and goal batch \n",
    "batch_size = next_observs.shape[0]\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# Get global variables needed for reward function\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch\n",
    "goal_orn_workframe_batch = model_env.goal_orn_workframe_batch\n",
    "\n",
    "print(reward(_, next_observs))\n",
    "print(model_env.reward(_, next_observs))\n",
    "print(any((reward(_, next_observs) == model_env.reward(_, next_observs))))\n",
    "\n",
    "# Get the global variables needed for termination function\n",
    "traj_pos_workframe = model_env.traj_pos_workframe\n",
    "targ_traj_list_id_batch = model_env.targ_traj_list_id_batch\n",
    "random_reward = reward(_, next_observs)\n",
    "\n",
    "# print(termination(_, next_observs))\n",
    "# print(model_env.termination(_, next_observs))\n",
    "# print(model_env.termination_fn(act, next_observs))\n",
    "print(all((termination(_, next_observs, random_reward) == model_env.termination(_, next_observs, random_reward))))\n",
    "# print(random_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation_action_sequences\n",
    "planning_horizon = agent.optimizer.horizon\n",
    "# initialise action sequence\n",
    "# action_lb = env.action_space.low.tolist()\n",
    "# action_ub = env.action_space.high.tolist()\n",
    "# initial_solution = (((torch.tensor(action_lb) + torch.tensor(action_ub)) / 2)\n",
    "#             .float()\n",
    "#         )\n",
    "# initial_solution = initial_solution.repeat((15, 1))\n",
    "# mu, dispersion = agent.optimizer.optimizer._init_population_params(initial_solution)\n",
    "# action_sequences = torch.zeros((500,) + initial_solution.shape)\n",
    "# action_sequences = agent.optimizer.optimizer._sample_population(mu, dispersion, action_sequences)\n",
    "# # print(initial_solution.shape)\n",
    "\n",
    "# create random action sequences\n",
    "initial_solution = torch.from_numpy(np.array([env.action_space.sample() for _ in range(planning_horizon)])).float().to(device)\n",
    "# print(initial_solution.shape)\n",
    "mu, dispersion = agent.optimizer.optimizer._init_population_params(initial_solution)\n",
    "action_sequences = torch.zeros((500,) + initial_solution.shape).to(device)\n",
    "action_sequences = agent.optimizer.optimizer._sample_population(mu, dispersion, action_sequences)\n",
    "\n",
    "# Initialise environment\n",
    "initial_state = env.reset()\n",
    "\n",
    "# evaluate sequences\n",
    "print(model_env.evaluate_action_sequences(action_sequences, initial_state, 20).shape)\n",
    "print(any(model_env.targ_traj_list_id_batch!=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states_from_obs(obs):\n",
    "    if \"goal_excluded\" in env.observation_mode:\n",
    "        if env.planar_states == True:\n",
    "            tcp_pos_workframe = np.zeros(3)\n",
    "            tcp_orn_workframe = np.zeros(4)\n",
    "            cur_obj_pos_workframe = np.zeros(3)\n",
    "            cur_obj_orn_workframe = np.zeros(4)\n",
    "\n",
    "            tcp_pos_workframe[0:2] = obs[0:2]\n",
    "            tcp_orn_workframe[2:4] = obs[2:4]\n",
    "            cur_obj_pos_workframe[0:2]= obs[4:6]\n",
    "            cur_obj_orn_workframe[2:4] = obs[6:8]\n",
    "        else:   \n",
    "            # tcp_pos_workframe = obs[0:3]\n",
    "            # tcp_orn_workframe = obs[0:4]\n",
    "            cur_obj_pos_workframe = obs[4:7]\n",
    "            # cur_obj_orn_workframe = obs[7:11]\n",
    "\n",
    "    else:\n",
    "        if env.planar_states == True: \n",
    "            # tcp_pos_to_goal_workframe = np.zeros(3)\n",
    "            # tcp_orn_to_goal_workframe = np.zeros(4)\n",
    "            cur_obj_pos_to_goal_workframe = np.zeros(3)\n",
    "            # cur_obj_orn_to_goal_workframe = np.zeros(4)\n",
    "\n",
    "            # tcp_pos_to_goal_workframe[0:2] = obs[0:2]\n",
    "            # tcp_orn_to_goal_workframe[2:4] = obs[0:2]\n",
    "            cur_obj_pos_to_goal_workframe[0:2]= obs[2:4]\n",
    "            # cur_obj_orn_to_goal_workframe[2:4] = obs[4:6]\n",
    "        else:\n",
    "            # tcp_pos_to_goal_workframe = obs[0:3]\n",
    "            # tcp_orn_to_goal_workframe = obs[0:4]\n",
    "            cur_obj_pos_to_goal_workframe = obs[4:7]\n",
    "            # cur_obj_orn_to_goal_workframe = obs[7:11]\n",
    "\n",
    "        # tcp_pos_workframe = obs[0:3] + env.goal_pos_workframe\n",
    "        cur_obj_pos_workframe = cur_obj_pos_to_goal_workframe + env.goal_pos_workframe\n",
    "\n",
    "    return tcp_pos_workframe, tcp_orn_workframe, cur_obj_pos_workframe, cur_obj_orn_workframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make directory for saving data\n",
    "# data_columns = ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'tcp_Rz', 'contact_Rz', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "# training_result_directory = os.path.join(work_dir, \"training_result\")\n",
    "# os.makedirs(training_result_directory, exist_ok=True)\n",
    "\n",
    "# def save_data(arg):\n",
    "#     initial_buffer = []\n",
    "#     (env, obs, action, next_obs, reward, done, info) = arg\n",
    "#     (tcp_pos_workframe, \n",
    "#     tcp_orn_workframe,\n",
    "#     cur_obj_pos_workframe, \n",
    "#     cur_obj_orn_workframe) = get_states_from_obs(env, obs)\n",
    "#     cur_obj_rpy_workframe = env._pb.getEulerFromQuaternion(cur_obj_orn_workframe)\n",
    "#     tcp_rpy_workframe = env._pb.getEulerFromQuaternion(cur_obj_orn_workframe)\n",
    "#     initial_buffer.append(np.hstack([0, \n",
    "#                                     0, \n",
    "#                                     0,\n",
    "#                                     tcp_pos_workframe, \n",
    "#                                     cur_obj_pos_workframe,\n",
    "#                                     tcp_rpy_workframe[2],\n",
    "#                                     cur_obj_rpy_workframe[2],\n",
    "#                                     env.goal_pos_workframe, \n",
    "#                                     reward, \n",
    "#                                     info[\"tip_in_contact\"],\n",
    "#                                     done]))\n",
    "#     pd.DataFrame(initial_buffer).to_csv(os.path.join(training_result_directory, \"{}_result.csv\".format(\"initial_buffer\")), mode='a', header=False)\n",
    "\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    initial_buffer_size, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length,\n",
    "    # callback=save_data,\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "# df = pd.read_csv(os.path.join(training_result_directory, 'initial_buffer_result.csv'), names = data_columns)\n",
    "# loss_contact = False\n",
    "# print(len(df.query(\"contact==@loss_contact\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [0.0]\n",
    "val_scores = [0.0]\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_axes(_axs, _frame, _text, _trial, _steps_trial, _plan_time, _train_time, _all_rewards, _goal_reached, _train_loss, _val_loss, force_update=False):\n",
    "    if not force_update and (_steps_trial % 10 != 0):\n",
    "        return\n",
    "    _axs[0].imshow(_frame)\n",
    "    _axs[0].set_xticks([])\n",
    "    _axs[0].set_yticks([])\n",
    "    _axs[0].tick_params(axis='x', colors='white')\n",
    "    _axs[0].tick_params(axis='y', colors='white')\n",
    "    _axs[1].clear()\n",
    "    _axs[1].set_xlim([0, num_trials + .1])\n",
    "    _axs[1].set_ylim([min(_all_rewards), 0])\n",
    "    _axs[1].set_xlabel(\"Trial\")\n",
    "    _axs[1].set_ylabel(\"Trial reward\")\n",
    "    _axs[1].xaxis.label.set_color('white')\n",
    "    _axs[1].yaxis.label.set_color('white')\n",
    "    _axs[1].tick_params(axis='x', colors='white')\n",
    "    _axs[1].tick_params(axis='y', colors='white')\n",
    "    _axs[1].plot(_all_rewards, 'bs-', _goal_reached, 'rs')\n",
    "    _text.set_text(\"Trial {}: {} steps\\nTrain Loss: {:.2f}\\nVal Loss: {:.3g}\\nPlan time: {:.2f} s/step\\nTrain time: {:.2f} s\".format(_trial + 1, _steps_trial, _train_loss, _val_loss, _plan_time, _train_time))\n",
    "    display.display(plt.gcf())  \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "text_kwargs = dict(ha='center', va='center', fontsize=28, color='C1')\n",
    "def update_axes_text(_axs, _trial, _steps_trial, _plan_time, _train_time, _all_rewards, _goal_reached, _train_loss, _val_loss, force_update=False):\n",
    "    if not force_update and (_steps_trial % 10 != 0):\n",
    "        return\n",
    "    _axs[0].clear()\n",
    "    _axs[0].text(0.5, 0.5, \n",
    "    \"Trial {}: {} steps\\nTrain Loss: {:.2f}\\nVal Loss: {:.3g}\\nPlan time: {:.2f} s/step\\nTrain time: {:.2f} s\".format(_trial + 1, _steps_trial, _train_loss, _val_loss, _plan_time, _train_time), **text_kwargs)\n",
    "    _axs[1].clear()\n",
    "    _axs[1].set_xlim([0, num_trials + .1])\n",
    "    _axs[1].set_ylim([min(_all_rewards), 0])\n",
    "    _axs[1].set_xlabel(\"Trial\")\n",
    "    _axs[1].set_ylabel(\"Trial reward\")\n",
    "    _axs[1].xaxis.label.set_color('white')\n",
    "    _axs[1].yaxis.label.set_color('white')\n",
    "    _axs[1].tick_params(axis='x', colors='white')\n",
    "    _axs[1].tick_params(axis='y', colors='white')\n",
    "    _axs[1].plot(_all_rewards, 'bs-', _goal_reached, 'rs')\n",
    "    display.display(plt.gcf())  \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "# Test plot function\n",
    "env.reset()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(130, 80, \"\")\n",
    "ax_text.set_color('white')\n",
    "all_rewards = np.random.randint(-500, 0, num_trials + 1)\n",
    "goal_reached = np.random.randint(-500, 0, num_trials + 1)\n",
    "steps = np.sort(np.random.randint(0,10000, len(all_rewards)))\n",
    "plan_time = np.random.rand()\n",
    "train_time = np.random.rand()\n",
    "# update_axes(axs,env.render(mode=\"rgb_array\"),  ax_text, 0, 0, plan_time, train_time, all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)\n",
    "update_axes_text(axs, 0, 0, plan_time, train_time, all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging folder\n",
    "print(f\"Results will be saved at {work_dir}.\")\n",
    "\n",
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "# Create visualization objects\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(130, 80, \"\")\n",
    "ax_text.set_color('white')\n",
    "\n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "total_steps = [0]\n",
    "goal_reached = [0]\n",
    "training_result = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "\n",
    "record_video = True\n",
    "record_video_frequency = 5\n",
    "\n",
    "for trial in range(num_trials):\n",
    "\n",
    "    # Reset \n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    done = False\n",
    "    trial_reward = 0.0\n",
    "    trial_pb_steps = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    # Record video\n",
    "    if record_video and (trial+1) % record_video_frequency == 0:\n",
    "        record_every_n_frames = 3\n",
    "        render_img = env.render(mode=\"rgb_array\")\n",
    "        render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(work_dir, \"training_policy_trial_{}.mp4\".format((trial+1))),\n",
    "            fourcc,\n",
    "            24.0,\n",
    "            render_img_size,\n",
    "        )\n",
    "        \n",
    "    update_axes_text(axs, trial, steps_trial, plan_time, train_time,\n",
    "        all_rewards, goal_reached, train_losses[-1], val_scores[-1])\n",
    "    # update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, plan_time, train_time,\n",
    "    #     all_rewards, goal_reached, train_losses[-1], val_scores[-1])\n",
    "    \n",
    "    tcp_pos_workframe, tcp_orn_workframe, cur_obj_pos_workframe, cur_obj_orn_workframe = get_states_from_obs(obs)\n",
    "    cur_obj_rpy_workframe = env._pb.getEulerFromQuaternion(cur_obj_orn_workframe)\n",
    "    tcp_rpy_workframe = env._pb.getEulerFromQuaternion(tcp_orn_workframe)\n",
    "    training_result.append(np.hstack([trial, \n",
    "                                    steps_trial, \n",
    "                                    trial_pb_steps,\n",
    "                                    tcp_pos_workframe, \n",
    "                                    cur_obj_pos_workframe,\n",
    "                                    tcp_rpy_workframe[2],\n",
    "                                    cur_obj_rpy_workframe[2],\n",
    "                                    env.goal_pos_workframe, \n",
    "                                    trial_reward, \n",
    "                                    False,\n",
    "                                    done]))\n",
    "    while not done:\n",
    "\n",
    "        if steps_trial == 0:\n",
    "            # --------------- Model Training -----------------\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "            )\n",
    "            \n",
    "            start_train_time = time.time()\n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=50, \n",
    "                patience=50, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "            train_time = time.time() - start_train_time\n",
    "\n",
    "            if work_dir is not None:\n",
    "                dynamics_model.save(str(work_dir))\n",
    "                replay_buffer.save(work_dir)\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        next_obs, reward, done, info = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        update_axes_text(axs, trial, steps_trial, plan_time, train_time,\n",
    "            all_rewards, goal_reached, train_losses[-1], val_scores[-1])\n",
    "        # update_axes(\n",
    "        #     axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, plan_time, train_time, \n",
    "        #     all_rewards, goal_reached,  train_losses[-1], val_scores[-1])\n",
    "\n",
    "        obs = next_obs\n",
    "        trial_reward += reward\n",
    "        trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "        steps_trial += 1\n",
    "\n",
    "        # Save data for plotting training performance\n",
    "        tcp_pos_workframe, tcp_orn_workframe, cur_obj_pos_workframe, cur_obj_orn_workframe = get_states_from_obs(obs)\n",
    "        cur_obj_rpy_workframe = env._pb.getEulerFromQuaternion(cur_obj_orn_workframe)\n",
    "        tcp_rpy_workframe = env._pb.getEulerFromQuaternion(tcp_orn_workframe)\n",
    "        training_result.append(np.hstack([trial,\n",
    "                                        steps_trial,\n",
    "                                        trial_pb_steps * env._sim_time_step,\n",
    "                                        tcp_pos_workframe, \n",
    "                                        cur_obj_pos_workframe, \n",
    "                                        tcp_rpy_workframe[2],\n",
    "                                        cur_obj_rpy_workframe[2],\n",
    "                                        env.goal_pos_workframe, \n",
    "                                        trial_reward, \n",
    "                                        info[\"tip_in_contact\"],\n",
    "                                        done]))\n",
    "        \n",
    "        # Record video at every n trials\n",
    "        if record_video and (trial+1) % record_video_frequency == 0 and steps_trial % record_every_n_frames == 0:\n",
    "            render_img = env.render(mode=\"rgb_array\")\n",
    "            render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "            out.write(render_img)\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "\n",
    "    all_rewards.append(trial_reward)\n",
    "    total_steps.append(steps_trial + total_steps[-1])\n",
    "\n",
    "    # save goal reached data during training\n",
    "    if env.single_goal_reached:\n",
    "        goal_reached.append(trial_reward)\n",
    "    else:\n",
    "        goal_reached.append(0)\n",
    "\n",
    "    # release video at every n trials\n",
    "    if record_video and (trial+1) % record_video_frequency == 0:\n",
    "        out.release()\n",
    "\n",
    "update_axes_text(axs, trial, steps_trial, plan_time, train_time,\n",
    "    all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)\n",
    "# update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, plan_time, train_time, \n",
    "#     all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(total_steps[1:], all_rewards[1:], 'bs-', total_steps[1:], goal_reached[1:], 'rs')\n",
    "ax.set_xlabel(\"Samples\")\n",
    "ax.set_ylabel(\"Trial reward\")\n",
    "fig.savefig(os.path.join(work_dir, \"output.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_push_plots(df, trials, directory):\n",
    "    loss_contact = False\n",
    "    for trial in range(trials):\n",
    "        fig_xy, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"tcp_x\"], df.query(\"trial==@trial\")[\"tcp_y\"], \"b-\", label='tcp psosition')\n",
    "        ax.plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_x\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_y\"], \"g+\", markersize=20)\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"contact_x\"], df.query(\"trial==@trial\")[\"contact_y\"], \"r-\", label='contact psosition')\n",
    "        ax.plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_x\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_y\"], \"gx\", markersize=20)\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"goal_x\"].iloc[0], df.query(\"trial==@trial\")[\"goal_y\"].iloc[0], \"x\", markersize=20, markeredgecolor=\"black\", label=\"goal position\")\n",
    "        \n",
    "        for i, rows in df.query(\"trial==@trial\").iterrows():\n",
    "            if i % 10 == 0:\n",
    "                tcp_x, tcp_y, tcp_Rz= rows[\"tcp_x\"], rows[\"tcp_y\"], rows[\"tcp_Rz\"]\n",
    "                tcp_dx, tcp_dy = 0.05 * np.cos(tcp_Rz), 0.05 * np.sin(tcp_Rz)\n",
    "                plt.arrow(tcp_x, tcp_y, tcp_dx, tcp_dy, color='b')\n",
    "                obj_x, obj_y, obj_Rz= rows[\"contact_x\"], rows[\"contact_y\"], rows[\"contact_Rz\"]\n",
    "                obj_dx, obj_dy = 0.05 * np.cos(obj_Rz), 0.05 * np.sin(obj_Rz)\n",
    "                plt.arrow(obj_x, obj_y, obj_dx, obj_dy, color='r')\n",
    "        \n",
    "        ax.set_xlabel(\"x workframe\")\n",
    "        ax.set_ylabel(\"y workframe\")\n",
    "        ax.set_xlim([env.robot.arm.TCP_lims[0, 0], env.robot.arm.TCP_lims[0, 1]])\n",
    "        ax.set_ylim([env.robot.arm.TCP_lims[1, 0], env.robot.arm.TCP_lims[1, 1]])\n",
    "        ax.legend()\n",
    "        fig_xy.savefig(os.path.join(directory, \"workframe_plot_trial_{}.png\".format(trial)))\n",
    "        plt.close(fig_xy)\n",
    "\n",
    "        fig_time_xy, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "        axs[0].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"tcp_x\"], \"bs\", label='tcp ')\n",
    "        axs[0].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_x\"], \"g+\", markersize=20)\n",
    "        axs[0].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"contact_x\"], \"rs\", label='contact')\n",
    "        axs[0].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_x\"], \"gx\", markersize=20)\n",
    "        axs[0].set_xlabel(\"Time steps (s)\")\n",
    "        axs[0].set_ylabel(\"x axis workframe\")\n",
    "        axs[0].set_ylim([env.robot.arm.TCP_lims[0, 0], env.robot.arm.TCP_lims[0, 1]])\n",
    "        axs[0].legend()\n",
    "        axs[1].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"tcp_y\"], \"bs\", label='tcp')\n",
    "        axs[1].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_y\"], \"g+\", markersize=20)\n",
    "        axs[1].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"contact_y\"], \"rs\", label='contact')\n",
    "        axs[1].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_y\"], \"gx\", markersize=20)\n",
    "        axs[1].set_xlabel(\"Time steps (s)\")\n",
    "        axs[1].set_ylabel(\"y axis workframe\")\n",
    "        axs[1].set_ylim([env.robot.arm.TCP_lims[1, 0], env.robot.arm.TCP_lims[1, 1]])\n",
    "        axs[1].legend()\n",
    "        fig_time_xy.savefig(os.path.join(directory, \"time_plot_trial_{}.png\".format(trial)))\n",
    "        plt.close(fig_time_xy)\n",
    "\n",
    "# Save data \n",
    "training_result = np.array(training_result)\n",
    "data_columns = ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'tcp_Rz', 'contact_Rz', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "df_training = pd.DataFrame(training_result, columns = data_columns)\n",
    "pd.DataFrame(training_result).to_csv(os.path.join(work_dir, \"training_results.csv\"))\n",
    "\n",
    "# Plot the training results\n",
    "training_result_directory = os.path.join(work_dir, \"training_result\")\n",
    "if not os.path.exists(training_result_directory):\n",
    "    os.mkdir(training_result_directory)\n",
    "else:\n",
    "    for filename in os.listdir(training_result_directory):\n",
    "        file_path = os.path.join(training_result_directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "plot_and_save_push_plots(df_training, num_trials, training_result_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main PETS loop\n",
    "num_test_trials = 10\n",
    "all_rewards = []\n",
    "evaluation_result = []\n",
    "goal_reached = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "save_vid = True\n",
    "render = True\n",
    "\n",
    "if save_vid:\n",
    "    record_every_n_frames = 1\n",
    "    render_img = env.render(mode=\"rgb_array\")\n",
    "    render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(work_dir, \"evaluated_policy.mp4\"),\n",
    "        fourcc,\n",
    "        24.0,\n",
    "        render_img_size,\n",
    "    )\n",
    "\n",
    "for trial in range(num_test_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    trial_reward = 0.0\n",
    "    trial_pb_steps = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    tcp_pos_workframe, _, _, _, _ = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "    cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "    evaluation_result.append(np.hstack([trial, \n",
    "                                        steps_trial, \n",
    "                                        trial_pb_steps,\n",
    "                                        tcp_pos_workframe, \n",
    "                                        cur_obj_pos_workframe, \n",
    "                                        env.goal_pos_workframe, \n",
    "                                        trial_reward, \n",
    "                                        False,\n",
    "                                        done]))\n",
    "    while not done:\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        action = agent.act(obs, **{})\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        if render:\n",
    "            render_img = env.render(mode=\"rgb_array\")\n",
    "        else:\n",
    "            render_img = None\n",
    "        \n",
    "        obs = next_obs\n",
    "        trial_reward += reward\n",
    "        trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "        steps_trial += 1\n",
    "\n",
    "        tcp_pos_workframe, _, _, _, _ = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "        cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "        evaluation_result.append(np.hstack([trial, \n",
    "                                            steps_trial, \n",
    "                                            trial_pb_steps * env._sim_time_step,\n",
    "                                            tcp_pos_workframe, \n",
    "                                            cur_obj_pos_workframe, \n",
    "                                            env.goal_pos_workframe, \n",
    "                                            trial_reward, \n",
    "                                            info[\"tip_in_contact\"],\n",
    "                                            done]))\n",
    "            \n",
    "         # use record_every_n_frames to reduce size sometimes\n",
    "        if save_vid and steps_trial % record_every_n_frames == 0:\n",
    "\n",
    "            # warning to enable rendering\n",
    "            if render_img is None:\n",
    "                sys.exit('Must be rendering to save video')\n",
    "\n",
    "            render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "            out.write(render_img)\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    print(\"Terminated at step {} with reward {}, goal reached: {}\".format(steps_trial, trial_reward, env.single_goal_reached))\n",
    "    all_rewards.append(trial_reward)\n",
    "\n",
    "    # save goal reached data during training\n",
    "    if env.single_goal_reached:\n",
    "        goal_reached.append(trial_reward)\n",
    "    else:\n",
    "        goal_reached.append(0)\n",
    "\n",
    "if save_vid:\n",
    "    out.release()\n",
    "\n",
    "print(\"The average reward over {} episodes is {}\".format(num_test_trials, np.mean(all_rewards)))\n",
    "\n",
    "# Save data \n",
    "evaluation_result = np.array(evaluation_result)\n",
    "df_evaluation = pd.DataFrame(evaluation_result, columns = data_columns)\n",
    "pd.DataFrame(evaluation_result).to_csv(os.path.join(work_dir, \"evaluation_results.csv\"))\n",
    "\n",
    "# plot evaluation results\n",
    "evaluation_result_directory = os.path.join(work_dir, \"evaluation_result\")\n",
    "if not os.path.exists(evaluation_result_directory):\n",
    "    os.mkdir(evaluation_result_directory)\n",
    "else:\n",
    "    for filename in os.listdir(evaluation_result_directory):\n",
    "        file_path = os.path.join(evaluation_result_directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "plot_and_save_push_plots(df_evaluation, num_test_trials, evaluation_result_directory)\n",
    "\n",
    "# Plot evaluation results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(all_rewards, 'bs-', goal_reached, 'rs')\n",
    "ax.set_xlabel(\"Trial\")\n",
    "ax.set_ylabel(\"Trial reward\")\n",
    "fig.savefig(os.path.join(work_dir, \"evaluation_output.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model learning and rollout predictions to see if code is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimisation iterations for CEM\n",
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    buffer_size, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "# Train model first\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr= 1e-3, weight_decay=5e-5)\n",
    "dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "    replay_buffer,\n",
    "    batch_size=cfg.overrides.model_batch_size,\n",
    "    val_ratio=cfg.overrides.validation_ratio,\n",
    "    ensemble_size=ensemble_size,\n",
    "    shuffle_each_epoch=True,\n",
    "    bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    ")\n",
    "\n",
    "start_train_time = time.time()\n",
    "model_trainer.train(\n",
    "    dataset_train, \n",
    "    dataset_val=dataset_val, \n",
    "    num_epochs=100, \n",
    "    patience=50, \n",
    "    callback=train_callback,\n",
    "    silent=True)\n",
    "train_time = time.time() - start_train_time\n",
    "\n",
    "print(\"Training time: \", train_time)\n",
    "print(\"Train Loss: {}, Val Loss: {}\".format(train_losses[-1], val_scores[-1]))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].set_xlabel(\"Total training epochs\")\n",
    "ax[0].set_ylabel(\"Training loss (avg. NLL)\")\n",
    "ax[0].xaxis.label.set_color('white')\n",
    "ax[0].yaxis.label.set_color('white')\n",
    "ax[0].tick_params(axis='x', colors='white')\n",
    "ax[0].tick_params(axis='y', colors='white')\n",
    "ax[1].plot(val_scores)\n",
    "ax[1].set_xlabel(\"Total training epochs\")\n",
    "ax[1].set_ylabel(\"Validation score (avg. MSE)\")\n",
    "ax[1].xaxis.label.set_color('white')\n",
    "ax[1].yaxis.label.set_color('white')\n",
    "ax[1].tick_params(axis='x', colors='white')\n",
    "ax[1].tick_params(axis='y', colors='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Test model one set of action sequences from buffer to see exploding -------\n",
    "# states still occur\n",
    "\n",
    "planning_horizon = 10\n",
    "\n",
    "# Get action sequence from buffer\n",
    "data = replay_buffer.get_all()\n",
    "action_sequences = data.act[0:planning_horizon,:]\n",
    "action_sequences = np.tile(action_sequences, (5,1,1)).astype(np.float32)\n",
    "action_sequences = torch.from_numpy(action_sequences)\n",
    "# print(action_sequences.shape)\n",
    "\n",
    "# Initialise state and create model input\n",
    "initial_state = data.obs[0]\n",
    "# print(initial_state.shape)\n",
    "initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "# print(initial_obs_batch.shape)\n",
    "model_state = model_env.reset(initial_obs_batch, return_as_np=False)\n",
    "# print(model_state['propagation_indices'])\n",
    "\n",
    "batch_size = initial_obs_batch.shape[0]\n",
    "total_rewards = torch.zeros(batch_size, 1)\n",
    "terminated = torch.zeros(batch_size, 1, dtype=bool)\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "print(data.obs[1][0:3])\n",
    "print(data.next_obs[1][0:3])\n",
    "print(data.act[1])\n",
    "\n",
    "for time_step in range(planning_horizon):\n",
    "    print(torch.mean(model_state[\"obs\"], 0)[0:3])\n",
    "    # print(model_state[\"obs\"].shape)\n",
    "    # print(torch.mean(model_state[\"obs\"]))\n",
    "    action_for_step = action_sequences[:, time_step, :]\n",
    "    # print(action_for_step[0])\n",
    "\n",
    "    # Re-initialise model state from data buffer with every time step (1 step rollouts)\n",
    "    # Comment out to do planning_horizon step rollouts\n",
    "    # initial_state = data.obs[time_step]\n",
    "    # initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "    # initial_obs_batch = torch.from_numpy(initial_obs_batch)\n",
    "    # model_state.update({'obs': initial_obs_batch})\n",
    "    # action_batch = torch.repeat_interleave(\n",
    "    #     action_for_step, 20, dim=0\n",
    "    # )\n",
    "\n",
    "    action_batch = action_for_step\n",
    "    # ---------------- Use model_env.step -----------------\n",
    "    # _, rewards, dones, model_state = model_env.step(\n",
    "    #     action_batch, model_state, sample=True\n",
    "    # )\n",
    "    # rewards[terminated] = 0\n",
    "    # terminated |= dones\n",
    "    # total_rewards += rewards\n",
    "\n",
    "    # -------------- Use one_dim_tr_model sample -------------\n",
    "    # with torch.no_grad():\n",
    "    #     next_observs, _, _, next_model_state, = model_env.dynamics_model.sample(\n",
    "    #         action_batch, model_state, deterministic=False, rng=model_env._rng,\n",
    "    #     )\n",
    "\n",
    "    # -------------- Use model.sample_1d() --------------------\n",
    "    # with torch.no_grad():\n",
    "    #     obs = model_state[\"obs\"]\n",
    "    #     model_in = model_env.dynamics_model._get_model_input(model_state[\"obs\"], action_batch)\n",
    "    #     next_observs, _ = model_env.dynamics_model.model.sample_1d(\n",
    "    #         model_in, model_state, rng=model_env._rng, deterministic=False\n",
    "    #     )\n",
    "    #     next_observs += obs\n",
    "    #     model_state[\"obs\"] = next_observs\n",
    "\n",
    "    # -------------- Use model.forward()-------------------------\n",
    "    with torch.no_grad():\n",
    "        obs = model_state[\"obs\"]\n",
    "        model_in = model_env.dynamics_model._get_model_input(model_state[\"obs\"], action_batch)\n",
    "        means, logvars = model_env.dynamics_model.model.forward(\n",
    "            model_in, rng=model_env._rng, propagation_indices=model_state[\"propagation_indices\"]\n",
    "        )\n",
    "        variances = logvars.exp()\n",
    "        stds = torch.sqrt(variances)\n",
    "        # stds = torch.ones((5,30))\n",
    "        next_observs = torch.normal(means, stds, generator=model_env._rng)\n",
    "        # next_observs = means\n",
    "        # print(torch.mean(means))\n",
    "        # print(torch.mean(logvars))\n",
    "        # print(torch.mean(stds))\n",
    "        if dynamics_model.target_normalizer:\n",
    "            next_observs = dynamics_model.target_normalizer.denormalize(next_observs)\n",
    "\n",
    "        if dynamics_model.target_is_delta:\n",
    "            next_observs += obs\n",
    "        model_state[\"obs\"] = next_observs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation  function\n",
    "def evaluate(env, agent, training_trial_num, work_dir):\n",
    "    \n",
    "    all_rewards = []\n",
    "    evaluation_result = []\n",
    "    goal_reached = []\n",
    "    plan_time = 0.0\n",
    "    train_time = 0.0\n",
    "    save_vid = True\n",
    "    render = True\n",
    "\n",
    "    if save_vid:\n",
    "        record_every_n_frames = 1\n",
    "        render_img = env.render(mode=\"rgb_array\")\n",
    "        render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(\n",
    "            os.path.join(work_dir, \"evaluated_policy_training_trial_{}.mp4\".format(training_trial_num)),\n",
    "            fourcc,\n",
    "            24.0,\n",
    "            render_img_size,\n",
    "        )\n",
    "\n",
    "    for trial in range(1):\n",
    "        obs = env.reset()    \n",
    "        agent.reset()\n",
    "        \n",
    "        done = False\n",
    "        trial_reward = 0.0\n",
    "        trial_pb_steps = 0.0\n",
    "        steps_trial = 0\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # --- Doing env step using the agent and adding to model dataset ---\n",
    "            start_plan_time = time.time()\n",
    "            action = agent.act(obs, **{})\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            plan_time = time.time() - start_plan_time\n",
    "\n",
    "            if render:\n",
    "                render_img = env.render(mode=\"rgb_array\")\n",
    "            else:\n",
    "                render_img = None\n",
    "            \n",
    "            obs = next_obs\n",
    "            trial_reward += reward\n",
    "            trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "            steps_trial += 1\n",
    "\n",
    "                \n",
    "            # use record_every_n_frames to reduce size sometimes\n",
    "            if save_vid and steps_trial % record_every_n_frames == 0:\n",
    "\n",
    "                # warning to enable rendering\n",
    "                if render_img is None:\n",
    "                    sys.exit('Must be rendering to save video')\n",
    "\n",
    "                render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "                out.write(render_img)\n",
    "\n",
    "            if steps_trial == trial_length:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(trial_reward)\n",
    "\n",
    "        # save goal reached data during training\n",
    "        if env.single_goal_reached:\n",
    "            goal_reached.append(trial_reward)\n",
    "        else:\n",
    "            goal_reached.append(0)\n",
    "\n",
    "    if save_vid:\n",
    "        out.release()\n",
    "\n",
    "# create a evaluation environment\n",
    "eval_env = gym.make(env_name, **env_kwargs)\n",
    "eval_env.seed(seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tactile_gym_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
