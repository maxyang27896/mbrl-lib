{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, shutil\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a display to render image\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Environment and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the pushing environment\n",
    "algo_name = 'ppo'\n",
    "env_name = 'object_push-v0'\n",
    "rl_params, algo_params, augmentations = import_parameters(env_name, algo_name)\n",
    "rl_params[\"env_modes\"][ 'observation_mode'] = 'tactile_pose_goal_excluded_data'\n",
    "rl_params[\"env_modes\"][ 'control_mode'] = 'TCP_position_control'\n",
    "# rl_params[\"env_modes\"]['movement_mode'] = 'TxTyRz'\n",
    "rl_params[\"env_modes\"][ 'terminate_early']  = True\n",
    "rl_params[\"env_modes\"][ 'use_contact'] = True\n",
    "rl_params[\"env_modes\"][ 'traj_type'] = 'point'\n",
    "rl_params[\"env_modes\"]['task'] = \"goal_pos\",\n",
    "# rl_params[\"env_modes\"]['planar_states'] = True\n",
    "print(rl_params)\n",
    "print(algo_params)\n",
    "\n",
    "env_kwargs={\n",
    "    'show_gui':False,\n",
    "    'show_tactile':False,\n",
    "    'max_steps':rl_params[\"max_ep_len\"],\n",
    "    'image_size':rl_params[\"image_size\"],\n",
    "    'env_modes':rl_params[\"env_modes\"],\n",
    "}\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape\n",
    "\n",
    "# This functions allows the model to evaluate the true rewards given an observation \n",
    "reward_fn = reward_fns.cartpole\n",
    "# This function allows the model to know if an observation should make the episode end\n",
    "term_fn = termination_fns.cartpole\n",
    "\n",
    "# Define model working directorys\n",
    "work_dir = os.path.join(os.getcwd(), 'saved_model')\n",
    "if not os.path.exists(work_dir):\n",
    "    os.mkdir(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.reset())\n",
    "print(env.get_observation())\n",
    "print(type(env.get_observation()))\n",
    "# print(env.get_tactile_pose_obs_goal_exluded())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(obs_shape)\n",
    "print(act_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 1000\n",
    "num_trials = 30\n",
    "ensemble_size = 5\n",
    "initial_buffer_size = 1000\n",
    "buffer_size = 10000\n",
    "target_normalised = True\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "        \"target_normalize\": target_normalised,\n",
    "        \"dataset_size\": buffer_size\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "\n",
    "print(dynamics_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    initial_buffer_size, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 10,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 500,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving config files\n",
    "config_filename = 'cfg_dict'\n",
    "config_dir = os.path.join(work_dir, config_filename)\n",
    "omegaconf.OmegaConf.save(config=cfg, f=config_dir) \n",
    "loaded = omegaconf.OmegaConf.load(config_dir)\n",
    "assert cfg == loaded\n",
    "\n",
    "agent_config_filename = 'agent_cfg'\n",
    "agent_config_dir = os.path.join(work_dir, agent_config_filename)\n",
    "omegaconf.OmegaConf.save(config=agent_cfg, f=agent_config_dir) \n",
    "loaded = omegaconf.OmegaConf.load(agent_config_dir)\n",
    "assert agent_cfg == loaded\n",
    "\n",
    "env_kwargs_filename = 'env_kwargs'\n",
    "env_kwargs_dir = os.path.join(work_dir, env_kwargs_filename)\n",
    "omegaconf.OmegaConf.save(config=env_kwargs, f=env_kwargs_dir) \n",
    "loaded = omegaconf.OmegaConf.load(env_kwargs_dir)\n",
    "assert env_kwargs == loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.update_trajectory()\n",
    "print(env.get_obj_pos_workframe())\n",
    "print(env.cur_obj_pos_worldframe)\n",
    "print(env.xyz_obj_dist_to_goal())\n",
    "env.reset()\n",
    "(\n",
    "    tcp_pos_workframe,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "print(tcp_pos_workframe)\n",
    "print(env.get_obj_pos_workframe())\n",
    "print(env.cur_obj_pos_worldframe)\n",
    "print(env.xyz_obj_dist_to_goal())\n",
    "for i in range(10):\n",
    "    action = np.zeros(3)\n",
    "    action[0] = -0.1\n",
    "    _, _, done, _ = env.step(action)\n",
    "    print(env._sim_time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "# ax_text = axs[0].text(130, 80, \"\")\n",
    "# ax_text.set_color('white')\n",
    "\n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "for trial in range(1):\n",
    "    obs = env.reset()    \n",
    "    # agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "    while not done:\n",
    "\n",
    "        # axs[0].imshow(env.render(mode=\"rgb_array\"))\n",
    "        # display.display(plt.gcf())  \n",
    "        # display.clear_output(wait=True)\n",
    "        \n",
    "        action = np.zeros(3)\n",
    "        action[0] = -0.1\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        # print(env.get_obj_pos_worldframe())\n",
    "        # print(env.get_obj_pos_workframe())\n",
    "        # print(env.xyz_obj_dist_to_goal())\n",
    "        # (\n",
    "        #     tcp_pos_workframe,\n",
    "        #     _,\n",
    "        #     _,\n",
    "        #     _,\n",
    "        #     _,\n",
    "        # ) = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "        # print(tcp_pos_workframe)\n",
    "        print(reward)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "\n",
    "print(env.get_obj_pos_worldframe())\n",
    "print(env.xyz_obj_dist_to_goal())\n",
    "print(env.cur_obj_pos_worldframe)\n",
    "print(env.targ_traj_list_id)\n",
    "print(env._env_step_counter)\n",
    "(\n",
    "    tcp_pos_workframe,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    "    _,\n",
    ") = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "print(tcp_pos_workframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(\"Current goal index\", env.targ_traj_list_id)\n",
    "print(\"Position goal trajectory\", env.traj_pos_workframe)\n",
    "# print(\"Orientation goal trajectory\", env.traj_rpy_workframe)\n",
    "# print(\"Orientation goal trajectory\", env.traj_orn_workframe)\n",
    "\n",
    "# All of this can be accessed through model_env.env\n",
    "print(model_env.env.targ_traj_list_id)\n",
    "print(model_env.env.traj_pos_workframe == env.traj_pos_workframe)\n",
    "print(model_env.env.goal_pos_workframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Test termination function in model env -------------------------\n",
    "early_termination = env.terminate_early\n",
    "def termination(act: torch.Tensor, next_obs: torch.Tensor, rewards:  torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Criteria for terminating an episode. Should return a vector of dones of size \n",
    "    population_size x batch_size\n",
    "    '''\n",
    "\n",
    "    if 'reduced' in env.observation_mode: \n",
    "        tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "        # tcp_orn_to_goal_workframe = next_obs[:, 3:7]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 7:10]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 10:13]\n",
    "        cur_obj_pos_to_goal_workframe = next_obs[:, 13:16]\n",
    "        # cur_obj_orn_to_goal_workframe = next_obs[:, 16:20]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 20:23]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 23:26]\n",
    "\n",
    "        tcp_pos_workframe = tcp_pos_to_goal_workframe + goal_pos_workframe_batch\n",
    "        cur_obj_pos_workframe = cur_obj_pos_to_goal_workframe + goal_pos_workframe_batch\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_to_goal_workframe, axis=1)\n",
    "    \n",
    "    elif env.observation_mode == 'tactile_pose_data': \n",
    "        tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "        # tcp_orn_to_goal_workframe = next_obs[:, 3:7]\n",
    "        cur_obj_pos_to_goal_workframe = next_obs[:, 7:10]\n",
    "        # cur_obj_orn_to_goal_workframe = next_obs[:, 10:14]\n",
    "\n",
    "        # Only take in the x and y coordinates\n",
    "        tcp_pos_workframe = tcp_pos_to_goal_workframe[:, 0:2] + goal_pos_workframe_batch[:, 0:2]\n",
    "        cur_obj_pos_workframe = cur_obj_pos_to_goal_workframe[:, 0:2] + goal_pos_workframe_batch[:, 0:2]\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_to_goal_workframe, axis=1)\n",
    "            \n",
    "    elif env.observation_mode == 'tactile_pose_goal_excluded_data': \n",
    "\n",
    "        if env.planar_states == True:\n",
    "            tcp_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float64).to(device)\n",
    "            # tcp_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float64).to(device)\n",
    "            cur_obj_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float64).to(device)\n",
    "            # cur_obj_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float64).to(device)\n",
    "\n",
    "            tcp_pos_workframe[:, 0:2] = next_obs[:, 0:2]\n",
    "            # tcp_orn_workframe[:, 2:4] = next_obs[:, 2:4]\n",
    "            cur_obj_pos_workframe[:, 0:2]= next_obs[:, 4:6]\n",
    "            # cur_obj_orn_workframe[:, 2:4] = next_obs[:, 6:8]\n",
    "        else:   \n",
    "\n",
    "            tcp_pos_workframe = next_obs[:, 0:3]\n",
    "            # tcp_orn_workframe = next_obs[:, 3:7]\n",
    "            cur_obj_pos_workframe = next_obs[:, 7:10]\n",
    "            # cur_obj_orn_workframe = next_obs[:, 10:14]\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_workframe - goal_pos_workframe_batch, axis=1)\n",
    "\n",
    "    else:\n",
    "        tcp_pos_workframe = next_obs[:, 0:3]\n",
    "        # tcp_rpy_workframe = next_obs[:, 3:6]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 6:9]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 9:12]\n",
    "        cur_obj_pos_workframe = next_obs[:, 12:15]\n",
    "        # cur_obj_rpy_workframe = next_obs[:, 15:18]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 18:21]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 21:24]\n",
    "        # pred_goal_pos_workframe = next_obs[:, 24:27]\n",
    "        # pred_goal_rpy_workframe = next_obs[:, 27:30]\n",
    "\n",
    "        # Calculate distance between goal and current positon\n",
    "        obj_goal_pos_dist = torch.linalg.norm(cur_obj_pos_workframe - goal_pos_workframe_batch, axis=1)\n",
    "    \n",
    "    # print('Object position, ', cur_obj_pos_workframe)\n",
    "    # print('TCP position, ', tcp_pos_workframe)\n",
    "    # print(obj_goal_pos_dist)\n",
    "\n",
    "    # Set obj to goal to smaller than tolerance for testing\n",
    "    # obj_goal_pos_dist[1] = 0.001\n",
    "\n",
    "    # intiailise terminated vector\n",
    "    terminated = torch.zeros((batch_size, 1), dtype=bool).to(device)\n",
    "\n",
    "    # print(\"goal position batch before update \\n\", goal_pos_workframe_batch)\n",
    "    # print(\"goal index batch before update\", targ_traj_list_id_batch)\n",
    "\n",
    "    # Early termination if outside of the tcp limits\n",
    "    if early_termination:\n",
    "        outside_tcp_lims_idx = outside_tcp_lims(tcp_pos_workframe, cur_obj_pos_workframe)\n",
    "        terminated[outside_tcp_lims_idx] = True\n",
    "        rewards[outside_tcp_lims_idx] -= env.terminated_early_penalty\n",
    "        # print(\"Outside TPC_lims, \", outside_tcp_lims(tcp_pos_workframe, cur_obj_pos_workframe))\n",
    "\n",
    "    # update goals index if subgoal reached\n",
    "    targ_traj_list_id_batch[obj_goal_pos_dist < model_env.termination_pos_dist] += 1\n",
    "\n",
    "    # Terminated is true if last subgoal is reached\n",
    "    terminated[targ_traj_list_id_batch >= model_env.traj_n_points] = True\n",
    "\n",
    "    # Update goal position batch for none terminated samples\n",
    "    goal_pos_workframe_batch[~terminated[:,0]] = traj_pos_workframe[targ_traj_list_id_batch[~terminated[:,0]]]\n",
    "\n",
    "    # print(\"obj to goal distance\", obj_goal_pos_dist)\n",
    "    # print(\"goal index batch\", targ_traj_list_id_batch)\n",
    "    # print(\"terminated batch\", terminated)\n",
    "    # print(\"goal position index not terminated\", targ_traj_list_id_batch[~terminated[:,0]])\n",
    "    # print(\"The none terminated goals to be updated\", traj_pos_workframe[targ_traj_list_id_batch[~terminated[:,0]]])\n",
    "    # print(\"The updated goals \\n\", goal_pos_workframe_batch)\n",
    "    return terminated\n",
    "\n",
    "def outside_tcp_lims(tcp_pos_workframe, cur_obj_pos_workframe):\n",
    "    # xyz_tcp_dist_to_obj = torch.linalg.norm(tcp_pos_workframe - cur_obj_pos_workframe)\n",
    "    return ((tcp_pos_workframe[:, 0] < env.robot.arm.TCP_lims[0,0]) | \n",
    "        (tcp_pos_workframe[:, 0] > env.robot.arm.TCP_lims[0,1]) | \n",
    "        (tcp_pos_workframe[:, 1] < env.robot.arm.TCP_lims[1,0]) | \n",
    "        (tcp_pos_workframe[:, 1] > env.robot.arm.TCP_lims[1,1]) | \n",
    "        (cur_obj_pos_workframe[:, 0] < env.robot.arm.TCP_lims[0,0]) | \n",
    "        (cur_obj_pos_workframe[:, 0] > env.robot.arm.TCP_lims[0,1]) | \n",
    "        (cur_obj_pos_workframe[:, 1] < env.robot.arm.TCP_lims[1,0]) | \n",
    "        (cur_obj_pos_workframe[:, 1] > env.robot.arm.TCP_lims[1,1])) \n",
    "        # (xyz_tcp_dist_to_obj > env.obj_width / 2))                # TODO: exiting episode when roughly lose contact\n",
    "\n",
    "\n",
    "# Reset environment\n",
    "batch_size = 3\n",
    "env.reset()\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# Create goal batches (access through model_env)\n",
    "traj_pos_workframe = model_env.traj_pos_workframe.clone()\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch.clone()\n",
    "targ_traj_list_id_batch = model_env.targ_traj_list_id_batch\n",
    "# targ_traj_list_id_batch = torch.from_numpy(targ_traj_list_id_batch).long()\n",
    "# targ_traj_list_id_batch[0] = 11\n",
    "\n",
    "obs = torch.randn(batch_size, obs_shape[0]).to(device)\n",
    "if \"reduced\" in env.observation_mode:\n",
    "    obs[:, :3] =  torch.tensor([[0.2, 0.05, 0.1], [0.2, 0.05, 0.1], [0.2, 0.05, 0.1]]) \n",
    "    obs[:, 13:16] = torch.tensor([[0.2, 0.05, 0.1], [0.2, 0.1, 0.1], [0.2, 0.05, 0.1]])\n",
    "elif env.observation_mode =='tactile_pose_data':\n",
    "    obs[:, 0:2] =  torch.tensor([[0.20, 0.01], [0.2, 0.01], [0.2, 0.01]]) \n",
    "    obs[:, 7:9] = torch.tensor([[0.2, 0.01], [0.20, 0.01], [0.2, 0.01]])\n",
    "elif env.observation_mode == 'tactile_pose_goal_excluded_data': \n",
    "    if env.planar_states == True:\n",
    "        obs[:, 0:2] =  torch.tensor([[0.2, 0.01], [0.2, 0.01], [0.2, 0.01]]) \n",
    "        obs[:, 4:6] = torch.tensor([[0.2, 0.01], [0.20, 0.10], [0.2, 0.01]])\n",
    "    else:\n",
    "        obs[:, 0:2] =  torch.tensor([[0.2, 0.01], [0.2, 0.01], [0.2, 0.01]]) \n",
    "        obs[:, 7:9] = torch.tensor([[0.2, 0.01], [0.20, 0.01], [0.2, 0.01]])\n",
    "\n",
    "else:\n",
    "    obs[:, :3] =  torch.tensor([[0.3, 0.1, 0.1], [0.3, 0.1, 0.1], [0.3, 0.1, 0.1]]) \n",
    "    obs[:, 12:15] = torch.tensor([[0.3, 0.1, 0.1], [0.3, 0.1, 0.1], [0.4, 0.1, 0.1]]) \n",
    "\n",
    "act = torch.randn(batch_size, 1).to(device)\n",
    "rewards_test = torch.zeros(batch_size, 1).to(device)\n",
    "rewards_push = torch.zeros(batch_size, 1).to(device)\n",
    "print(termination(act, obs, rewards_test))\n",
    "print(model_env.termination(act, obs, rewards_push))\n",
    "print(rewards_test)\n",
    "print(rewards_push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Test early termination ------------------------\n",
    "early_termination = True\n",
    "\n",
    "tcp_pos_workframe = torch.tensor([[0.2, 0.05, 0.1], [0.2, 0.05, 0.1]]) \n",
    "cur_obj_pos_workframe = torch.tensor([[0.2, 0.15, 0.1], [0.2, 0.05, 0.1]]) # The first sample for obj_pos is out of y_lims\n",
    "print(outside_tcp_lims(tcp_pos_workframe, cur_obj_pos_workframe))\n",
    "\n",
    "batch_size = 2\n",
    "env.reset()\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# Create goal batches (access through model_env)\n",
    "traj_pos_workframe = model_env.traj_pos_workframe\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch\n",
    "targ_traj_list_id_batch = model_env.targ_traj_list_id_batch\n",
    "\n",
    "obs = torch.randn(batch_size, 30).to(device)\n",
    "act = torch.randn(batch_size, 1).to(device)\n",
    "rewards = torch.randn(batch_size).to(device)\n",
    "if \"reduced\" in env.observation_mode:\n",
    "    obs[:, 0:3] = tcp_pos_workframe\n",
    "    obs[:, 13:16] = cur_obj_pos_workframe\n",
    "elif \"tactile_pose\" in env.observation_mode:\n",
    "    if env.planar_states == True:\n",
    "        obs[:, 0:2] = tcp_pos_workframe[:, 0:2]\n",
    "        obs[:, 4:6] = cur_obj_pos_workframe[:, 0:2]\n",
    "    else:\n",
    "        obs[:, 0:3] = tcp_pos_workframe\n",
    "        obs[:, 7:10] = cur_obj_pos_workframe\n",
    "else:\n",
    "    obs[:, 0:3] = tcp_pos_workframe\n",
    "    obs[:, 12:15] = cur_obj_pos_workframe\n",
    "print(termination(_, obs, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Test reward function  in model env-------------------------------\n",
    "from mbrl.util.math import euler_to_quaternion, quaternion_rotation_matrix\n",
    "\n",
    "def xyz_obj_dist_to_goal(cur_obj_pos_workframe):\n",
    "\n",
    "    # obj to goal distance\n",
    "    return torch.linalg.norm(cur_obj_pos_workframe - goal_pos_workframe_batch, axis=1)\n",
    "\n",
    "def get_pos_dist(pos_dist_vector):\n",
    "\n",
    "    # obj to goal distance\n",
    "    return torch.linalg.norm(pos_dist_vector, axis=1)\n",
    "\n",
    "def orn_obj_dist_to_goal_rpy(cur_obj_rpy_workframe):\n",
    "    cur_obj_orn_workframe = euler_to_quaternion(cur_obj_rpy_workframe)\n",
    "    inner_product = torch.sum(goal_orn_workframe_batch*cur_obj_orn_workframe, 1)\n",
    "    return torch.arccos(torch.clip(2 * (inner_product ** 2) - 1, -1, 1))\n",
    "\n",
    "def orn_obj_dist_to_goal_orn(cur_obj_orn_workframe):\n",
    "    inner_product = torch.sum(goal_orn_workframe_batch*cur_obj_orn_workframe, 1)\n",
    "    return torch.arccos(torch.clip(2 * (inner_product ** 2) - 1, -1, 1))\n",
    "\n",
    "def get_orn_dist(orn_dist_vector):\n",
    "    \"\"\"\n",
    "    Distance between the current obj orientation and goal orientation.\n",
    "    \"\"\"\n",
    "    dist = torch.arccos(torch.clip(\n",
    "        (2 * (orn_dist_vector[:, 3]**2)) - 1, -1, 1))\n",
    "\n",
    "    # other option is to get \n",
    "    # cur_obj_orn_workframe = cur_obj_orn_to_goal_workframe * goal_orn_workframe_batch\n",
    "    return dist\n",
    "\n",
    "def cos_tcp_dist_to_obj(cur_obj_rpy_workframe, tcp_rpy_workframe):\n",
    "    \"\"\"\n",
    "    Cos distance from current orientation of the TCP to the current\n",
    "    orientation of the object\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = cur_obj_rpy_workframe.shape[0]\n",
    "\n",
    "    # tip normal to object normal\n",
    "    cur_obj_orn_workframe = euler_to_quaternion(cur_obj_rpy_workframe)\n",
    "    obj_rot_matrix_workframe = quaternion_rotation_matrix(cur_obj_orn_workframe)\n",
    "    obj_rot_matrix_workframe = torch.reshape(obj_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    obj_init_vector_workframe = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32).to(device)\n",
    "    obj_vector_workframe = torch.matmul(obj_rot_matrix_workframe, obj_init_vector_workframe)\n",
    "    # obj_vector_workframe = obj_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    tcp_orn_workframe = euler_to_quaternion(tcp_rpy_workframe)\n",
    "    tip_rot_matrix_workframe = quaternion_rotation_matrix(tcp_orn_workframe)\n",
    "    tip_rot_matrix_workframe  = torch.reshape(tip_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    tip_init_vector_workframe  = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32).to(device)\n",
    "    tip_vector_workframe  = torch.matmul(tip_rot_matrix_workframe, tip_init_vector_workframe)\n",
    "    # tip_vector_workframe = tip_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    obj_tip_dot_product = torch.sum(obj_vector_workframe*tip_vector_workframe, 1)\n",
    "    cos_sim_workfrfame = obj_tip_dot_product / (\n",
    "        torch.linalg.norm(obj_vector_workframe, axis=1) * torch.linalg.norm(tip_vector_workframe, axis=1)\n",
    "    )\n",
    "    cos_dist_workframe = 1 - cos_sim_workfrfame\n",
    "\n",
    "    return cos_dist_workframe\n",
    "\n",
    "def cos_tcp_dist_to_obj_reduced(cur_obj_orn_to_goal_workframe, tcp_orn_to_goal_workframe):\n",
    "    \"\"\"\n",
    "    Cos distance from current orientation of the TCP to the current\n",
    "    orientation of the object\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = cur_obj_orn_to_goal_workframe.shape[0]\n",
    "\n",
    "    # tip normal to object normal\n",
    "    obj_rot_matrix_workframe = quaternion_rotation_matrix(cur_obj_orn_to_goal_workframe)\n",
    "    obj_rot_matrix_workframe = torch.reshape(obj_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    # obj_init_vector_workframe = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)\n",
    "    # obj_vector_workframe = torch.matmul(obj_rot_matrix_workframe, obj_init_vector_workframe)\n",
    "    obj_vector_workframe = obj_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    tip_rot_matrix_workframe = quaternion_rotation_matrix(tcp_orn_to_goal_workframe)\n",
    "    tip_rot_matrix_workframe  = torch.reshape(tip_rot_matrix_workframe, (batch_size, 3, 3))\n",
    "    # tip_init_vector_workframe  = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)\n",
    "    # tip_vector_workframe  = torch.matmul(tip_rot_matrix_workframe, tip_init_vector_workframe)\n",
    "    tip_vector_workframe = tip_rot_matrix_workframe[:, :, 0]\n",
    "\n",
    "    obj_tip_dot_product = torch.sum(obj_vector_workframe*tip_vector_workframe, 1)\n",
    "    cos_sim_workfrfame = obj_tip_dot_product / (\n",
    "        torch.linalg.norm(obj_vector_workframe, axis=1) * torch.linalg.norm(tip_vector_workframe, axis=1)\n",
    "    )\n",
    "    cos_dist_workframe = 1 - cos_sim_workfrfame \n",
    "\n",
    "    return cos_dist_workframe\n",
    "\n",
    "def cos_tcp_Rz_dist_to_obj(cos_cur_obj_Rz_to_goal_workframe, cos_tcp_Rz_to_goal_workframe):\n",
    "    cos_sim_workframe = torch.cos(\n",
    "        torch.arccos(cos_tcp_Rz_to_goal_workframe) - torch.arccos(cos_cur_obj_Rz_to_goal_workframe)\n",
    "        )\n",
    "    return 1 - cos_sim_workframe\n",
    "\n",
    "def reward(act: torch.Tensor, next_obs: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Caculate the reward given a batch of observations \n",
    "    '''\n",
    "\n",
    "    batch_size = next_obs.shape[0]\n",
    "\n",
    "    if 'reduced' in env.observation_mode: \n",
    "        # tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "        tcp_orn_to_goal_workframe = next_obs[:, 3:7]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 7:10]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 10:13]\n",
    "        cur_obj_pos_to_goal_workframe = next_obs[:, 13:16]\n",
    "        cur_obj_orn_to_goal_workframe = next_obs[:, 16:20]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 20:23]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 23:26]\n",
    "\n",
    "        obj_goal_pos_dist = get_pos_dist(cur_obj_pos_to_goal_workframe)\n",
    "        obj_goal_orn_dist = get_orn_dist(cur_obj_orn_to_goal_workframe)\n",
    "        tip_obj_orn_dist = cos_tcp_dist_to_obj_reduced(cur_obj_orn_to_goal_workframe, tcp_orn_to_goal_workframe)\n",
    "\n",
    "        print(obj_goal_orn_dist)\n",
    "    elif env.observation_mode == 'tactile_pose_data': \n",
    "        tcp_pos_to_goal_workframe = next_obs[:, 0:3]\n",
    "        tcp_orn_to_goal_workframe = next_obs[:, 3:7]\n",
    "        cur_obj_pos_to_goal_workframe = next_obs[:, 7:10]\n",
    "        cur_obj_orn_to_goal_workframe = next_obs[:, 10:14]\n",
    "\n",
    "        obj_goal_pos_dist = get_pos_dist(cur_obj_pos_to_goal_workframe)\n",
    "        obj_goal_orn_dist = get_orn_dist(cur_obj_orn_to_goal_workframe)\n",
    "        tip_obj_orn_dist = cos_tcp_dist_to_obj_reduced(cur_obj_orn_to_goal_workframe, tcp_orn_to_goal_workframe)\n",
    "\n",
    "    elif env.observation_mode == 'tactile_pose_goal_excluded_data':\n",
    "        if env.planar_states == True:\n",
    "            # tcp_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            tcp_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "            cur_obj_pos_workframe = torch.zeros((len(next_obs), 3), dtype=torch.float32).to(device)\n",
    "            cur_obj_orn_workframe = torch.zeros((len(next_obs), 4), dtype=torch.float32).to(device)\n",
    "\n",
    "            # tcp_pos_workframe[:, 0:2] = next_obs[:, 0:2]\n",
    "            tcp_orn_workframe[:, 2:4] = next_obs[:, 2:4]\n",
    "            cur_obj_pos_workframe[:, 0:2]= next_obs[:, 4:6]\n",
    "            cur_obj_orn_workframe[:, 2:4] = next_obs[:, 6:8]\n",
    "        else:   \n",
    "            # tcp_pos_workframe = next_obs[:, 0:3]\n",
    "            tcp_orn_workframe = next_obs[:, 3:7]\n",
    "            cur_obj_pos_workframe = next_obs[:, 7:10]\n",
    "            cur_obj_orn_workframe = next_obs[:, 10:14]\n",
    "\n",
    "        obj_goal_pos_dist = xyz_obj_dist_to_goal(cur_obj_pos_workframe)\n",
    "        obj_goal_orn_dist = orn_obj_dist_to_goal_orn(cur_obj_orn_workframe)\n",
    "        tip_obj_orn_dist = cos_tcp_dist_to_obj_reduced(cur_obj_orn_workframe, tcp_orn_workframe)\n",
    "        \n",
    "    else:\n",
    "        # tcp_pos_workframe = next_obs[:, 0:3]\n",
    "        tcp_rpy_workframe = next_obs[:, 3:6]\n",
    "        # tcp_lin_vel_workframe = next_obs[:, 6:9]\n",
    "        # tcp_ang_vel_workframe = next_obs[:, 9:12]\n",
    "        cur_obj_pos_workframe = next_obs[:, 12:15]\n",
    "        cur_obj_rpy_workframe = next_obs[:, 15:18]\n",
    "        # cur_obj_lin_vel_workframe = next_obs[:, 18:21]\n",
    "        # cur_obj_ang_vel_workframe = next_obs[:, 21:24]\n",
    "        # pred_goal_pos_workframe = next_obs[:, 24:27]\n",
    "        # pred_goal_rpy_workframe = next_obs[:, 27:30]\n",
    "\n",
    "        obj_goal_pos_dist = xyz_obj_dist_to_goal(cur_obj_pos_workframe)\n",
    "        obj_goal_orn_dist = orn_obj_dist_to_goal_rpy(cur_obj_rpy_workframe)\n",
    "        tip_obj_orn_dist = cos_tcp_dist_to_obj(cur_obj_rpy_workframe, tcp_rpy_workframe)\n",
    "\n",
    "    reward = -(obj_goal_pos_dist + obj_goal_orn_dist + tip_obj_orn_dist)\n",
    "    reward = reward[:, None]\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Create observation and goal batch \n",
    "batch_size = 2\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "    obs, _, done, _ = env.step(env.action_space.sample())\n",
    "obs = torch.tensor(obs).to(torch.float32).to(device)\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "obs_batch = torch.tile(obs, (batch_size,) + tuple([1] * obs.ndim))\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch\n",
    "goal_orn_workframe_batch = model_env.goal_orn_workframe_batch\n",
    "\n",
    "if env.observation_mode == 'tactile_pose_data': \n",
    "    print(env.dense_reward_tactile_pose())\n",
    "else:\n",
    "    print(env.dense_reward())\n",
    "print(reward(_, obs_batch))\n",
    "print(model_env.reward(_, obs_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reward and terminal function using next_observ from model.sample()\n",
    "planning_horizon = agent.optimizer.horizon\n",
    "# initialise action sequence\n",
    "action_lb = env.action_space.low.tolist()\n",
    "action_ub = env.action_space.high.tolist()\n",
    "initial_solution = (((torch.tensor(action_lb) + torch.tensor(action_ub)) / 2)\n",
    "            .float().to(device)\n",
    "        )\n",
    "initial_solution = initial_solution.repeat((planning_horizon, 1))\n",
    "mu, dispersion = agent.optimizer.optimizer._init_population_params(initial_solution)\n",
    "action_sequences = torch.zeros((500,) + initial_solution.shape).to(device)\n",
    "action_sequences = agent.optimizer.optimizer._sample_population(mu, dispersion, action_sequences)\n",
    "\n",
    "# Intialise state and create model state for model input\n",
    "initial_state = env.reset()\n",
    "tiling_shape = (20 * 500,) + tuple(\n",
    "    [1] * initial_state.ndim\n",
    ")\n",
    "initial_obs_batch = np.tile(initial_state, tiling_shape).astype(np.float32)\n",
    "model_state = model_env.reset(initial_obs_batch, return_as_np=False)\n",
    "\n",
    "# get action for time step\n",
    "action_for_step = action_sequences[:, 0, :]\n",
    "action_batch = torch.repeat_interleave(\n",
    "                    action_for_step, 20, dim=0\n",
    "                )\n",
    "\n",
    "# Get next observation from model\n",
    "(\n",
    "    next_observs,\n",
    "    pred_rewards,\n",
    "    pred_terminals,\n",
    "    next_model_state,\n",
    ") = model_env.dynamics_model.sample(\n",
    "    action_batch,\n",
    "    model_state,\n",
    "    deterministic=False,\n",
    "    rng=model_env._rng,\n",
    ")\n",
    "\n",
    "# Next obervation types\n",
    "# print(next_observs.type())\n",
    "# print(next_observs.dtype)\n",
    "# print(next_observs.shape)\n",
    "\n",
    "# Create observation and goal batch \n",
    "batch_size = next_observs.shape[0]\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# Get global variables needed for reward function\n",
    "\n",
    "# If taken from env, need to turn into tensor and float 32\n",
    "# If taken from model_env, the goals are already converted\n",
    "# goal_pos_workframe = torch.tensor(env.goal_pos_workframe).to(torch.float32)\n",
    "# goal_orn_workframe = torch.tensor(env.goal_orn_workframe).to(torch.float32)\n",
    "# goal_pos_workframe = model_env.goal_pos_workframe\n",
    "# goal_orn_workframe = model_env.goal_orn_workframe\n",
    "# goal_pos_workframe_batch = torch.tile(goal_pos_workframe, (batch_size,) + tuple([1] * goal_pos_workframe.ndim))\n",
    "# goal_orn_workframe_batch = torch.tile(goal_orn_workframe, (batch_size,) + tuple([1] * goal_orn_workframe.ndim))\n",
    "goal_pos_workframe_batch = model_env.goal_pos_workframe_batch\n",
    "goal_orn_workframe_batch = model_env.goal_orn_workframe_batch\n",
    "\n",
    "print(reward(_, next_observs))\n",
    "print(model_env.reward(_, next_observs))\n",
    "# print(model_env.reward_fn(_, next_observs))\n",
    "print(any((reward(_, next_observs) == model_env.reward(_, next_observs))))\n",
    "\n",
    "# Get the global variables needed for termination function\n",
    "traj_pos_workframe = model_env.traj_pos_workframe\n",
    "targ_traj_list_id_batch = model_env.targ_traj_list_id_batch\n",
    "random_reward = reward(_, next_observs)\n",
    "\n",
    "# print(termination(_, next_observs))\n",
    "# print(model_env.termination(_, next_observs))\n",
    "# print(model_env.termination_fn(act, next_observs))\n",
    "print(all((termination(_, next_observs, random_reward) == model_env.termination(_, next_observs, random_reward))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation_action_sequences\n",
    "planning_horizon = agent.optimizer.horizon\n",
    "# initialise action sequence\n",
    "# action_lb = env.action_space.low.tolist()\n",
    "# action_ub = env.action_space.high.tolist()\n",
    "# initial_solution = (((torch.tensor(action_lb) + torch.tensor(action_ub)) / 2)\n",
    "#             .float()\n",
    "#         )\n",
    "# initial_solution = initial_solution.repeat((15, 1))\n",
    "# mu, dispersion = agent.optimizer.optimizer._init_population_params(initial_solution)\n",
    "# action_sequences = torch.zeros((500,) + initial_solution.shape)\n",
    "# action_sequences = agent.optimizer.optimizer._sample_population(mu, dispersion, action_sequences)\n",
    "# # print(initial_solution.shape)\n",
    "\n",
    "# create random action sequences\n",
    "initial_solution = torch.from_numpy(np.array([env.action_space.sample() for _ in range(planning_horizon)])).float().to(device)\n",
    "# print(initial_solution.shape)\n",
    "mu, dispersion = agent.optimizer.optimizer._init_population_params(initial_solution)\n",
    "action_sequences = torch.zeros((500,) + initial_solution.shape).to(device)\n",
    "action_sequences = agent.optimizer.optimizer._sample_population(mu, dispersion, action_sequences)\n",
    "\n",
    "# Initialise environment\n",
    "initial_state = env.reset()\n",
    "\n",
    "# evaluate sequences\n",
    "print(model_env.evaluate_action_sequences(action_sequences, initial_state, 20).shape)\n",
    "print(any(model_env.targ_traj_list_id_batch!=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [0.0]\n",
    "val_scores = [0.0]\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())   # this returns val score per ensemble model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_axes(_axs, _frame, _text, _trial, _steps_trial, _plan_time, _train_time, _all_rewards, _goal_reached, _train_loss, _val_loss, force_update=False):\n",
    "    if not force_update and (_steps_trial % 10 != 0):\n",
    "        return\n",
    "    _axs[0].imshow(_frame)\n",
    "    _axs[0].set_xticks([])\n",
    "    _axs[0].set_yticks([])\n",
    "    _axs[0].tick_params(axis='x', colors='white')\n",
    "    _axs[0].tick_params(axis='y', colors='white')\n",
    "    _axs[1].clear()\n",
    "    _axs[1].set_xlim([0, num_trials + .1])\n",
    "    _axs[1].set_ylim([min(_all_rewards), 0])\n",
    "    _axs[1].set_xlabel(\"Trial\")\n",
    "    _axs[1].set_ylabel(\"Trial reward\")\n",
    "    _axs[1].xaxis.label.set_color('white')\n",
    "    _axs[1].yaxis.label.set_color('white')\n",
    "    _axs[1].tick_params(axis='x', colors='white')\n",
    "    _axs[1].tick_params(axis='y', colors='white')\n",
    "    _axs[1].plot(_all_rewards, 'bs-', _goal_reached, 'rs')\n",
    "    _text.set_text(\"Trial {}: {} steps\\nTrain Loss: {:.2f}\\nVal Loss: {:.3g}\\nPlan time: {:.2f} s/step\\nTrain time: {:.2f} s\".format(_trial + 1, _steps_trial, _train_loss, _val_loss, _plan_time, _train_time))\n",
    "    display.display(plt.gcf())  \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "text_kwargs = dict(ha='center', va='center', fontsize=28, color='C1')\n",
    "def update_axes_text(_axs, _trial, _steps_trial, _plan_time, _train_time, _all_rewards, _goal_reached, _train_loss, _val_loss, force_update=False):\n",
    "    if not force_update and (_steps_trial % 10 != 0):\n",
    "        return\n",
    "    _axs[0].clear()\n",
    "    _axs[0].text(0.5, 0.5, \n",
    "    \"Trial {}: {} steps\\nTrain Loss: {:.2f}\\nVal Loss: {:.3g}\\nPlan time: {:.2f} s/step\\nTrain time: {:.2f} s\".format(_trial + 1, _steps_trial, _train_loss, _val_loss, _plan_time, _train_time), **text_kwargs)\n",
    "    _axs[1].clear()\n",
    "    _axs[1].set_xlim([0, num_trials + .1])\n",
    "    _axs[1].set_ylim([min(_all_rewards), 0])\n",
    "    _axs[1].set_xlabel(\"Trial\")\n",
    "    _axs[1].set_ylabel(\"Trial reward\")\n",
    "    _axs[1].xaxis.label.set_color('white')\n",
    "    _axs[1].yaxis.label.set_color('white')\n",
    "    _axs[1].tick_params(axis='x', colors='white')\n",
    "    _axs[1].tick_params(axis='y', colors='white')\n",
    "    _axs[1].plot(_all_rewards, 'bs-', _goal_reached, 'rs')\n",
    "    display.display(plt.gcf())  \n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "# Test plot function\n",
    "env.reset()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(130, 80, \"\")\n",
    "ax_text.set_color('white')\n",
    "all_rewards = np.random.randint(-500, 0, num_trials + 1)\n",
    "goal_reached = np.random.randint(-500, 0, num_trials + 1)\n",
    "steps = np.sort(np.random.randint(0,10000, len(all_rewards)))\n",
    "plan_time = np.random.rand()\n",
    "train_time = np.random.rand()\n",
    "# update_axes(axs,env.render(mode=\"rgb_array\"),  ax_text, 0, 0, plan_time, train_time, all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)\n",
    "update_axes_text(axs, 0, 0, plan_time, train_time, all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states_from_obs(obs):\n",
    "    if \"goal_excluded\" in env.observation_mode:\n",
    "        if env.planar_states == True:\n",
    "            tcp_pos_workframe = np.zeros(3)\n",
    "            # tcp_orn_workframe = np.zeros(4)\n",
    "            cur_obj_pos_workframe = np.zeros(3)\n",
    "            # cur_obj_orn_workframe = np.zeros(4)\n",
    "\n",
    "            tcp_pos_workframe[0:2] = obs[0:2]\n",
    "            # tcp_orn_workframe[2:4] = obs[2:4]\n",
    "            cur_obj_pos_workframe[0:2]= obs[4:6]\n",
    "            # cur_obj_orn_workframe[:, 2:4] = obs[6:8]\n",
    "        else:   \n",
    "            tcp_pos_workframe = obs[0:3]\n",
    "            # tcp_orn_workframe = obs[3:7]\n",
    "            cur_obj_pos_workframe = obs[7:10]\n",
    "            # cur_obj_orn_workframe = obs[10:14]\n",
    "    else:\n",
    "        tcp_pos_workframe = obs[0:3] + env.goal_pos_workframe\n",
    "        cur_obj_pos_workframe = obs[7:10] + env.goal_pos_workframe\n",
    "\n",
    "    return tcp_pos_workframe, cur_obj_pos_workframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging folder\n",
    "print(f\"Results will be saved at {work_dir}.\")\n",
    "\n",
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "# Create visualization objects\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "ax_text = axs[0].text(130, 80, \"\")\n",
    "ax_text.set_color('white')\n",
    "\n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "total_steps = [0]\n",
    "goal_reached = [0]\n",
    "training_result = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "for trial in range(num_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    done = False\n",
    "    trial_reward = 0.0\n",
    "    trial_pb_steps = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    update_axes_text(axs, trial, steps_trial, plan_time, train_time,\n",
    "        all_rewards, goal_reached, train_losses[-1], val_scores[-1])\n",
    "    # update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, plan_time, train_time,\n",
    "    #     all_rewards, goal_reached, train_losses[-1], val_scores[-1])\n",
    "\n",
    "    tcp_pos_workframe, cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "    training_result.append(np.hstack([trial, \n",
    "                                    steps_trial, \n",
    "                                    trial_pb_steps,\n",
    "                                    tcp_pos_workframe, \n",
    "                                    cur_obj_pos_workframe, \n",
    "                                    env.goal_pos_workframe, \n",
    "                                    trial_reward, \n",
    "                                    False,\n",
    "                                    done]))\n",
    "    while not done:\n",
    "\n",
    "        if steps_trial == 0:\n",
    "            # --------------- Model Training -----------------\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "            )\n",
    "            \n",
    "            start_train_time = time.time()\n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=50, \n",
    "                patience=50, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "            train_time = time.time() - start_train_time\n",
    "\n",
    "            if work_dir is not None:\n",
    "                dynamics_model.save(str(work_dir))\n",
    "                replay_buffer.save(work_dir)\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        next_obs, reward, done, info = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        update_axes_text(axs, trial, steps_trial, plan_time, train_time,\n",
    "            all_rewards, goal_reached, train_losses[-1], val_scores[-1])\n",
    "        # update_axes(\n",
    "        #     axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, plan_time, train_time, \n",
    "        #     all_rewards, goal_reached,  train_losses[-1], val_scores[-1])\n",
    "\n",
    "        obs = next_obs\n",
    "        trial_reward += reward\n",
    "        trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "        steps_trial += 1\n",
    "\n",
    "        # Save data for plotting training performance\n",
    "        tcp_pos_workframe, cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "        training_result.append(np.hstack([trial,\n",
    "                                        steps_trial,\n",
    "                                        trial_pb_steps * env._sim_time_step,\n",
    "                                        tcp_pos_workframe, \n",
    "                                        cur_obj_pos_workframe, \n",
    "                                        env.goal_pos_workframe, \n",
    "                                        trial_reward, \n",
    "                                        info[\"tip_in_contact\"],\n",
    "                                        done]))\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "\n",
    "    all_rewards.append(trial_reward)\n",
    "    total_steps.append(steps_trial + total_steps[-1])\n",
    "\n",
    "    # save goal reached data during training\n",
    "    if env.single_goal_reached:\n",
    "        goal_reached.append(trial_reward)\n",
    "    else:\n",
    "        goal_reached.append(0)\n",
    "\n",
    "update_axes_text(axs, trial, steps_trial, plan_time, train_time,\n",
    "    all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)\n",
    "# update_axes(axs, env.render(mode=\"rgb_array\"), ax_text, trial, steps_trial, plan_time, train_time, \n",
    "#     all_rewards, goal_reached, train_losses[-1], val_scores[-1], force_update=True)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(total_steps[1:], all_rewards[1:], 'bs-', total_steps[1:], goal_reached[1:], 'rs')\n",
    "ax.set_xlabel(\"Samples\")\n",
    "ax.set_ylabel(\"Trial reward\")\n",
    "fig.savefig(os.path.join(work_dir, \"output.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_push_plots(df, trials, directory):\n",
    "    loss_contact = False\n",
    "    for trial in range(trials):\n",
    "        fig_xy, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"tcp_x\"], df.query(\"trial==@trial\")[\"tcp_y\"], \"bs\", label='tcp psosition')\n",
    "        ax.plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_x\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_y\"], \"g+\", markersize=20)\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"contact_x\"], df.query(\"trial==@trial\")[\"contact_y\"], \"rs\", label='contact psosition')\n",
    "        ax.plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_x\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_y\"], \"gx\", markersize=20)\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"goal_x\"].iloc[0], df.query(\"trial==@trial\")[\"goal_y\"].iloc[0], \"x\", markersize=20, markeredgecolor=\"black\", label=\"goal position\")\n",
    "        ax.set_xlabel(\"x workframe\")\n",
    "        ax.set_ylabel(\"y workframe\")\n",
    "        ax.set_xlim([0, 0.3])\n",
    "        ax.set_ylim([-0.1, 0.1])\n",
    "        ax.legend()\n",
    "        fig_xy.savefig(os.path.join(directory, \"workframe_plot_trial_{}.png\".format(trial)))\n",
    "        plt.close(fig_xy)\n",
    "\n",
    "        fig_time_xy, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "        axs[0].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"tcp_x\"] - df.query(\"trial==@trial\")[\"goal_x\"], \"bs\", label='tcp from goal')\n",
    "        axs[0].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_x\"] - df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"goal_x\"], \"g+\", markersize=20)\n",
    "        axs[0].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"contact_x\"] - df.query(\"trial==@trial\")[\"goal_x\"], \"rs\", label='contact from goal')\n",
    "        axs[0].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_x\"]- df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"goal_x\"], \"gx\", markersize=20)\n",
    "        axs[0].set_xlabel(\"Time steps (s)\")\n",
    "        axs[0].set_ylabel(\"x axis workframe\")\n",
    "        axs[0].set_ylim([-0.3, 0.1])\n",
    "        axs[0].legend()\n",
    "        axs[1].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"tcp_y\"] - df.query(\"trial==@trial\")[\"goal_y\"], \"bs\", label='tcp')\n",
    "        axs[1].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_y\"] - df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"goal_y\"], \"g+\", markersize=20)\n",
    "        axs[1].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"contact_y\"] - df.query(\"trial==@trial\")[\"goal_y\"], \"rs\", label='contact from goal')\n",
    "        axs[1].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_y\"] - df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"goal_y\"], \"gx\", markersize=20)\n",
    "        axs[1].set_xlabel(\"Time steps (s)\")\n",
    "        axs[1].set_ylabel(\"y axis workframe\")\n",
    "        axs[1].set_ylim([-0.2, 0.2])\n",
    "        axs[1].legend()\n",
    "        fig_time_xy.savefig(os.path.join(directory, \"time_plot_trial_{}.png\".format(trial)))\n",
    "        plt.close(fig_time_xy)\n",
    "\n",
    "# Save data \n",
    "training_result = np.array(training_result)\n",
    "data_columns = ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "df_training = pd.DataFrame(training_result, columns = data_columns)\n",
    "pd.DataFrame(training_result).to_csv(os.path.join(work_dir, \"training_results.csv\"))\n",
    "\n",
    "# Plot the training results\n",
    "training_result_directory = os.path.join(work_dir, \"training_result\")\n",
    "if not os.path.exists(training_result_directory):\n",
    "    os.mkdir(training_result_directory)\n",
    "else:\n",
    "    for filename in os.listdir(training_result_directory):\n",
    "        file_path = os.path.join(training_result_directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "plot_and_save_push_plots(df_training, num_trials, training_result_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated at step 373 with reward -71.88854450774443, goal reached: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb Cell 28'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=41'>42</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=42'>43</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=43'>44</a>\u001b[0m     \u001b[39m# --- Doing env step using the agent and adding to model dataset ---\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=44'>45</a>\u001b[0m     start_plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=45'>46</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(obs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{})\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=46'>47</a>\u001b[0m     next_obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_pets_example.ipynb#ch0000026vscode-remote?line=47'>48</a>\u001b[0m     plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_plan_time\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:684\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act\u001b[0;34m(self, obs, optimizer_callback, **_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_eval_fn(obs, action_sequences)\n\u001b[1;32m    683\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 684\u001b[0m plan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    685\u001b[0m     trajectory_eval_fn, callback\u001b[39m=\u001b[39;49moptimizer_callback\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions_to_use\u001b[39m.\u001b[39mextend([a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m plan[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq]])\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:558\u001b[0m, in \u001b[0;36mTrajectoryOptimizer.optimize\u001b[0;34m(self, trajectory_eval_fn, callback)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     trajectory_eval_fn: Callable[[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    542\u001b[0m     callback: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    544\u001b[0m     \u001b[39m\"\"\"Runs the trajectory optimization.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39m        (tuple of np.ndarray and float): the best action sequence.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m     best_solution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    559\u001b[0m         trajectory_eval_fn,\n\u001b[1;32m    560\u001b[0m         x0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprevious_solution,\n\u001b[1;32m    561\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_last_solution:\n\u001b[1;32m    564\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_solution \u001b[39m=\u001b[39m best_solution\u001b[39m.\u001b[39mroll(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq, dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:172\u001b[0m, in \u001b[0;36mCEMOptimizer.optimize\u001b[0;34m(self, obj_fun, x0, callback, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_iterations):\n\u001b[1;32m    171\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_population(mu, dispersion, population)\n\u001b[0;32m--> 172\u001b[0m     values \u001b[39m=\u001b[39m obj_fun(population)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         callback(population, values, i)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:681\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act.<locals>.trajectory_eval_fn\u001b[0;34m(action_sequences)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(action_sequences):\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrajectory_eval_fn(obs, action_sequences)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:744\u001b[0m, in \u001b[0;36mcreate_trajectory_optim_agent_for_model.<locals>.trajectory_eval_fn\u001b[0;34m(initial_state, action_sequences)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(initial_state, action_sequences):\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m model_env\u001b[39m.\u001b[39;49mevaluate_action_sequences(\n\u001b[1;32m    745\u001b[0m         action_sequences, initial_state\u001b[39m=\u001b[39;49minitial_state, num_particles\u001b[39m=\u001b[39;49mnum_particles\n\u001b[1;32m    746\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:522\u001b[0m, in \u001b[0;36mModelEnvPushing.evaluate_action_sequences\u001b[0;34m(self, action_sequences, initial_state, num_particles)\u001b[0m\n\u001b[1;32m    518\u001b[0m action_for_step \u001b[39m=\u001b[39m action_sequences[:, time_step, :]\n\u001b[1;32m    519\u001b[0m action_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    520\u001b[0m     action_for_step, num_particles, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    521\u001b[0m )\n\u001b[0;32m--> 522\u001b[0m _, rewards, dones, model_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    523\u001b[0m     action_batch, model_state, sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    524\u001b[0m )\n\u001b[1;32m    525\u001b[0m rewards[terminated] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    526\u001b[0m terminated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m dones\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:139\u001b[0m, in \u001b[0;36mModelEnvPushing.step\u001b[0;34m(self, actions, model_state, sample)\u001b[0m\n\u001b[1;32m    124\u001b[0m     actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(actions)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    125\u001b[0m (\n\u001b[1;32m    126\u001b[0m     next_observs,\n\u001b[1;32m    127\u001b[0m     pred_rewards,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     rng\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rng,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m rewards \u001b[39m=\u001b[39m (\n\u001b[1;32m    137\u001b[0m     pred_rewards\n\u001b[1;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_fn(actions, next_observs)\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtermination_fn(actions, next_observs, rewards)\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m pred_terminals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:456\u001b[0m, in \u001b[0;36mModelEnvPushing.reward\u001b[0;34m(self, act, next_obs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     obj_goal_pos_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxyz_obj_dist_to_goal(cur_obj_pos_workframe)\n\u001b[1;32m    455\u001b[0m     obj_goal_orn_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morn_obj_dist_to_goal(cur_obj_orn_workframe)\n\u001b[0;32m--> 456\u001b[0m     tip_obj_orn_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_tcp_dist_to_obj(cur_obj_orn_workframe, tcp_orn_workframe)\n\u001b[1;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# tcp_pos_workframe = next_obs[:, 0:3]\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     tcp_rpy_workframe \u001b[39m=\u001b[39m next_obs[:, \u001b[39m3\u001b[39m:\u001b[39m6\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:362\u001b[0m, in \u001b[0;36mModelEnvPushing.cos_tcp_dist_to_obj\u001b[0;34m(self, obj_orn_data_workframe, tcp_orn_data_workframe)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m# tip normal to object normal\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m# In reduced the orientation data is querternion\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mreduced\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtactile_pose\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_mode: \n\u001b[0;32m--> 362\u001b[0m     obj_rot_matrix_workframe \u001b[39m=\u001b[39m quaternion_rotation_matrix(obj_orn_data_workframe)\n\u001b[1;32m    363\u001b[0m     tip_rot_matrix_workframe \u001b[39m=\u001b[39m quaternion_rotation_matrix(tcp_orn_data_workframe)\n\u001b[1;32m    364\u001b[0m \u001b[39m# In other obersavation mode the orientation data is euler\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/util/math.py:547\u001b[0m, in \u001b[0;36mquaternion_rotation_matrix\u001b[0;34m(quaternion)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquaternion_rotation_matrix\u001b[39m(quaternion: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    536\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39m    Covert a batch of quaternion into a full three-dimensional rotation matrix.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39m \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39m             frame to a point in the global reference frame.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     d \u001b[39m=\u001b[39m quaternion[:, \u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39;49m quaternion[:, \u001b[39m0\u001b[39;49m] \u001b[39m+\u001b[39;49m quaternion[:, \u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39;49m quaternion[:, \u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m quaternion[:, \u001b[39m2\u001b[39;49m] \u001b[39m*\u001b[39;49m quaternion[:, \u001b[39m2\u001b[39;49m] \u001b[39m+\u001b[39m quaternion[:, \u001b[39m3\u001b[39m] \u001b[39m*\u001b[39m quaternion[:, \u001b[39m3\u001b[39m]\n\u001b[1;32m    548\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m d\n\u001b[1;32m    549\u001b[0m     xs \u001b[39m=\u001b[39m quaternion[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m s\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main PETS loop\n",
    "num_test_trials = 20\n",
    "all_rewards = []\n",
    "evaluation_result = []\n",
    "goal_reached = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "save_vid = True\n",
    "render = True\n",
    "\n",
    "if save_vid:\n",
    "    record_every_n_frames = 1\n",
    "    render_img = env.render(mode=\"rgb_array\")\n",
    "    render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(work_dir, \"evaluated_policy.mp4\"),\n",
    "        fourcc,\n",
    "        24.0,\n",
    "        render_img_size,\n",
    "    )\n",
    "\n",
    "for trial in range(num_test_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    trial_reward = 0.0\n",
    "    trial_pb_steps = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    tcp_pos_workframe, cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "    evaluation_result.append(np.hstack([trial, \n",
    "                                        steps_trial, \n",
    "                                        trial_pb_steps,\n",
    "                                        tcp_pos_workframe, \n",
    "                                        cur_obj_pos_workframe, \n",
    "                                        env.goal_pos_workframe, \n",
    "                                        trial_reward, \n",
    "                                        False,\n",
    "                                        done]))\n",
    "    while not done:\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        action = agent.act(obs, **{})\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        if render:\n",
    "            render_img = env.render(mode=\"rgb_array\")\n",
    "        else:\n",
    "            render_img = None\n",
    "        \n",
    "        obs = next_obs\n",
    "        trial_reward += reward\n",
    "        trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "        steps_trial += 1\n",
    "\n",
    "        tcp_pos_workframe, cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "        evaluation_result.append(np.hstack([trial, \n",
    "                                            steps_trial, \n",
    "                                            trial_pb_steps * env._sim_time_step,\n",
    "                                            tcp_pos_workframe, \n",
    "                                            cur_obj_pos_workframe, \n",
    "                                            env.goal_pos_workframe, \n",
    "                                            trial_reward, \n",
    "                                            info[\"tip_in_contact\"],\n",
    "                                            done]))\n",
    "            \n",
    "         # use record_every_n_frames to reduce size sometimes\n",
    "        if save_vid and steps_trial % record_every_n_frames == 0:\n",
    "\n",
    "            # warning to enable rendering\n",
    "            if render_img is None:\n",
    "                sys.exit('Must be rendering to save video')\n",
    "\n",
    "            render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "            out.write(render_img)\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    print(\"Terminated at step {} with reward {}, goal reached: {}\".format(steps_trial, trial_reward, env.single_goal_reached))\n",
    "    all_rewards.append(trial_reward)\n",
    "\n",
    "    # save goal reached data during training\n",
    "    if env.single_goal_reached:\n",
    "        goal_reached.append(trial_reward)\n",
    "    else:\n",
    "        goal_reached.append(0)\n",
    "\n",
    "if save_vid:\n",
    "    out.release()\n",
    "\n",
    "print(\"The average reward over {} episodes is {}\".format(num_test_trials, np.mean(all_rewards)))\n",
    "\n",
    "# Save data \n",
    "evaluation_result = np.array(evaluation_result)\n",
    "df_evaluation = pd.DataFrame(evaluation_result, columns = data_columns)\n",
    "pd.DataFrame(evaluation_result).to_csv(os.path.join(work_dir, \"evaluation_results.csv\"))\n",
    "\n",
    "# plot evaluation results\n",
    "evaluation_result_directory = os.path.join(work_dir, \"evaluation_result\")\n",
    "if not os.path.exists(evaluation_result_directory):\n",
    "    os.mkdir(evaluation_result_directory)\n",
    "else:\n",
    "    for filename in os.listdir(evaluation_result_directory):\n",
    "        file_path = os.path.join(evaluation_result_directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "plot_and_save_push_plots(df_evaluation, num_test_trials, evaluation_result_directory)\n",
    "\n",
    "# Plot evaluation results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(all_rewards, 'bs-', goal_reached, 'rs')\n",
    "ax.set_xlabel(\"Trial\")\n",
    "ax.set_ylabel(\"Trial reward\")\n",
    "fig.savefig(os.path.join(work_dir, \"evaluation_output.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model learning and rollout predictions to see if code is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimisation iterations for CEM\n",
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    buffer_size, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)\n",
    "\n",
    "# Train model first\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr= 1e-3, weight_decay=5e-5)\n",
    "dynamics_model.update_normalizer(replay_buffer.get_all())\n",
    "dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "    replay_buffer,\n",
    "    batch_size=cfg.overrides.model_batch_size,\n",
    "    val_ratio=cfg.overrides.validation_ratio,\n",
    "    ensemble_size=ensemble_size,\n",
    "    shuffle_each_epoch=True,\n",
    "    bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    ")\n",
    "\n",
    "start_train_time = time.time()\n",
    "model_trainer.train(\n",
    "    dataset_train, \n",
    "    dataset_val=dataset_val, \n",
    "    num_epochs=100, \n",
    "    patience=50, \n",
    "    callback=train_callback,\n",
    "    silent=True)\n",
    "train_time = time.time() - start_train_time\n",
    "\n",
    "print(\"Training time: \", train_time)\n",
    "print(\"Train Loss: {}, Val Loss: {}\".format(train_losses[-1], val_scores[-1]))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].set_xlabel(\"Total training epochs\")\n",
    "ax[0].set_ylabel(\"Training loss (avg. NLL)\")\n",
    "ax[0].xaxis.label.set_color('white')\n",
    "ax[0].yaxis.label.set_color('white')\n",
    "ax[0].tick_params(axis='x', colors='white')\n",
    "ax[0].tick_params(axis='y', colors='white')\n",
    "ax[1].plot(val_scores)\n",
    "ax[1].set_xlabel(\"Total training epochs\")\n",
    "ax[1].set_ylabel(\"Validation score (avg. MSE)\")\n",
    "ax[1].xaxis.label.set_color('white')\n",
    "ax[1].yaxis.label.set_color('white')\n",
    "ax[1].tick_params(axis='x', colors='white')\n",
    "ax[1].tick_params(axis='y', colors='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Test model one set of action sequences from buffer to see exploding -------\n",
    "# states still occur\n",
    "\n",
    "planning_horizon = 20\n",
    "\n",
    "# Get action sequence from buffer\n",
    "data = replay_buffer.get_all()\n",
    "action_sequences = data.act[0:planning_horizon,:]\n",
    "action_sequences = np.tile(action_sequences, (5,1,1)).astype(np.float32)\n",
    "action_sequences = torch.from_numpy(action_sequences)\n",
    "# print(action_sequences.shape)\n",
    "\n",
    "# Initialise state and create model input\n",
    "initial_state = data.obs[0]\n",
    "# print(initial_state.shape)\n",
    "initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "# print(initial_obs_batch.shape)\n",
    "model_state = model_env.reset(initial_obs_batch, return_as_np=False)\n",
    "# print(model_state['propagation_indices'])\n",
    "\n",
    "batch_size = initial_obs_batch.shape[0]\n",
    "total_rewards = torch.zeros(batch_size, 1)\n",
    "terminated = torch.zeros(batch_size, 1, dtype=bool)\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "print(data.obs[1][0:3])\n",
    "print(data.next_obs[1][0:3])\n",
    "print(data.act[1])\n",
    "\n",
    "for time_step in range(planning_horizon):\n",
    "    print(torch.mean(model_state[\"obs\"], 0)[0:3])\n",
    "    # print(model_state[\"obs\"].shape)\n",
    "    # print(torch.mean(model_state[\"obs\"]))\n",
    "    action_for_step = action_sequences[:, time_step, :]\n",
    "    # print(action_for_step[0])\n",
    "\n",
    "    # Re-initialise model state from data buffer with every time step (1 step rollouts)\n",
    "    # Comment out to do planning_horizon step rollouts\n",
    "    # initial_state = data.obs[time_step]\n",
    "    # initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "    # initial_obs_batch = torch.from_numpy(initial_obs_batch)\n",
    "    # model_state.update({'obs': initial_obs_batch})\n",
    "    # action_batch = torch.repeat_interleave(\n",
    "    #     action_for_step, 20, dim=0\n",
    "    # )\n",
    "\n",
    "    action_batch = action_for_step\n",
    "    # ---------------- Use model_env.step -----------------\n",
    "    # _, rewards, dones, model_state = model_env.step(\n",
    "    #     action_batch, model_state, sample=True\n",
    "    # )\n",
    "    # rewards[terminated] = 0\n",
    "    # terminated |= dones\n",
    "    # total_rewards += rewards\n",
    "\n",
    "    # -------------- Use one_dim_tr_model sample -------------\n",
    "    # with torch.no_grad():\n",
    "    #     next_observs, _, _, next_model_state, = model_env.dynamics_model.sample(\n",
    "    #         action_batch, model_state, deterministic=False, rng=model_env._rng,\n",
    "    #     )\n",
    "\n",
    "    # -------------- Use model.sample_1d() --------------------\n",
    "    # with torch.no_grad():\n",
    "    #     obs = model_state[\"obs\"]\n",
    "    #     model_in = model_env.dynamics_model._get_model_input(model_state[\"obs\"], action_batch)\n",
    "    #     next_observs, _ = model_env.dynamics_model.model.sample_1d(\n",
    "    #         model_in, model_state, rng=model_env._rng, deterministic=False\n",
    "    #     )\n",
    "    #     next_observs += obs\n",
    "    #     model_state[\"obs\"] = next_observs\n",
    "\n",
    "    # -------------- Use model.forward()-------------------------\n",
    "    with torch.no_grad():\n",
    "        obs = model_state[\"obs\"]\n",
    "        model_in = model_env.dynamics_model._get_model_input(model_state[\"obs\"], action_batch)\n",
    "        means, logvars = model_env.dynamics_model.model.forward(\n",
    "            model_in, rng=model_env._rng, propagation_indices=model_state[\"propagation_indices\"]\n",
    "        )\n",
    "        variances = logvars.exp()\n",
    "        stds = torch.sqrt(variances)\n",
    "        # stds = torch.ones((5,30))\n",
    "        next_observs = torch.normal(means, stds, generator=model_env._rng)\n",
    "        # next_observs = means\n",
    "        # print(torch.mean(means))\n",
    "        # print(torch.mean(logvars))\n",
    "        # print(torch.mean(stds))\n",
    "        if dynamics_model.target_normalizer:\n",
    "            next_observs = dynamics_model.target_normalizer.denormalize(next_observs)\n",
    "\n",
    "        if dynamics_model.target_is_delta:\n",
    "            next_observs += obs\n",
    "        model_state[\"obs\"] = next_observs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tactile_gym_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
