{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a display to render image\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/saved_model\n"
     ]
    }
   ],
   "source": [
    "# Define model working directorys\n",
    "work_dir = os.path.join(os.getcwd(), 'saved_model')\n",
    "print(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar  8 2021 17:26:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    }
   ],
   "source": [
    "# Load the environment \n",
    "env_name = 'object_push-v0'\n",
    "env_kwargs_file = 'env_kwargs'\n",
    "env_kwargs_dir = os.path.join(work_dir, env_kwargs_file)\n",
    "env_kwargs = omegaconf.OmegaConf.load(env_kwargs_dir)\n",
    "\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    }
   ],
   "source": [
    "# Get cfg and agent cfg\n",
    "config_file = 'cfg_dict'\n",
    "config_dir = os.path.join(work_dir, config_file)\n",
    "cfg = omegaconf.OmegaConf.load(config_dir)\n",
    "\n",
    "trial_length= cfg.overrides.trial_length\n",
    "\n",
    "agent_config_file = 'agent_cfg'\n",
    "agent_config_dir = os.path.join(os.getcwd(), 'saved_model', agent_config_file)\n",
    "agent_cfg = omegaconf.OmegaConf.load(agent_config_dir)\n",
    "\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "\n",
    "# Create dyanmics, replay buffer and agent \n",
    "dynamics_model.load(work_dir)\n",
    "replay_buffer.load(work_dir)\n",
    "dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats  \n",
    "\n",
    "# Create agent \n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated at step 313 with reward -17.38547286976729\n",
      "Terminated at step 323 with reward -20.981221880911402\n",
      "Terminated at step 278 with reward -13.781116926288457\n",
      "Terminated at step 283 with reward -12.460440634563174\n",
      "Terminated at step 277 with reward -16.775125955865523\n",
      "Terminated at step 332 with reward -25.751804064195333\n",
      "Terminated at step 296 with reward -17.557747981812188\n",
      "Terminated at step 311 with reward -34.156947666046996\n",
      "Terminated at step 292 with reward -16.698085011985043\n",
      "Terminated at step 326 with reward -26.761177336998706\n",
      "The average reward over 10 episodes is -20.23091403284341\n"
     ]
    }
   ],
   "source": [
    "# Main PETS loop\n",
    "num_test_trials = 10\n",
    "all_rewards = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "save_vid = True\n",
    "render = True\n",
    "\n",
    "if save_vid:\n",
    "    record_every_n_frames = 1\n",
    "    render_img = env.render(mode=\"rgb_array\")\n",
    "    render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(work_dir, \"evaluated_policy.mp4\"),\n",
    "        fourcc,\n",
    "        24.0,\n",
    "        render_img_size,\n",
    "    )\n",
    "\n",
    "for trial in range(num_test_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "    while not done:\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        action = agent.act(obs, **{})\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        if render:\n",
    "            render_img = env.render(mode=\"rgb_array\")\n",
    "        else:\n",
    "            render_img = None\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        \n",
    "         # use record_every_n_frames to reduce size sometimes\n",
    "        if save_vid and steps_trial % record_every_n_frames == 0:\n",
    "\n",
    "            # warning to enable rendering\n",
    "            if render_img is None:\n",
    "                sys.exit('Must be rendering to save video')\n",
    "\n",
    "            render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "            out.write(render_img)\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    print(\"Terminated at step {} with reward {}\".format(steps_trial, total_reward))\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "if save_vid:\n",
    "    out.release()\n",
    "\n",
    "print(\"The average reward over {} episodes is {}\".format(num_test_trials, np.mean(all_rewards)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tactile_gym_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
