{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a display to render image\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qt21590/Documents/Projects/tactile_gym_mbrl/models/tactile_gym_mbrl/pose_contact_random_goal_position_4\n"
     ]
    }
   ],
   "source": [
    "# Define model working directorys\n",
    "# work_dir = os.path.join(os.getcwd(), 'saved_model')\n",
    "work_dir = r\"/home/qt21590/Documents/Projects/tactile_gym_mbrl/models/tactile_gym_mbrl/pose_contact_random_goal_position_4\"\n",
    "print(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) UHD Graphics (TGL GT1)\n",
      "GL_VERSION=4.6 (Compatibility Profile) Mesa 21.2.6\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "Version = 4.6 (Compatibility Profile) Mesa 21.2.6\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) UHD Graphics (TGL GT1)\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n"
     ]
    }
   ],
   "source": [
    "# Load the environment \n",
    "env_name = 'object_push-v0'\n",
    "env_kwargs_file = 'env_kwargs'\n",
    "env_kwargs_dir = os.path.join(work_dir, env_kwargs_file)\n",
    "env_kwargs = omegaconf.OmegaConf.load(env_kwargs_dir)\n",
    "\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    }
   ],
   "source": [
    "# Get cfg and agent cfg\n",
    "config_file = 'cfg_dict'\n",
    "config_dir = os.path.join(work_dir, config_file)\n",
    "cfg = omegaconf.OmegaConf.load(config_dir)\n",
    "trial_length= cfg.overrides.trial_length\n",
    "\n",
    "agent_config_file = 'agent_cfg'\n",
    "agent_config_dir = os.path.join(work_dir, agent_config_file)\n",
    "agent_cfg = omegaconf.OmegaConf.load(agent_config_dir)\n",
    "\n",
    "# Re-map device\n",
    "map_location = None\n",
    "if cfg['dynamics_model']['device'] != device:\n",
    "    cfg['dynamics_model']['device'] = device\n",
    "    agent_cfg['optimizer_cfg']['device'] = device\n",
    "    map_location = torch.device(device)\n",
    "    \n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "\n",
    "# Create dyanmics, replay buffer and agent \n",
    "dynamics_model.load(work_dir, map_location=map_location)\n",
    "replay_buffer.load(work_dir)\n",
    "dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats  \n",
    "\n",
    "# Create agent \n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=31'>32</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=32'>33</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=33'>34</a>\u001b[0m     \u001b[39m# --- Doing env step using the agent and adding to model dataset ---\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=34'>35</a>\u001b[0m     start_plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=35'>36</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(obs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=36'>37</a>\u001b[0m     next_obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#ch0000005?line=37'>38</a>\u001b[0m     plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_plan_time\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:684\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act\u001b[0;34m(self, obs, optimizer_callback, **_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_eval_fn(obs, action_sequences)\n\u001b[1;32m    683\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 684\u001b[0m plan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    685\u001b[0m     trajectory_eval_fn, callback\u001b[39m=\u001b[39;49moptimizer_callback\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions_to_use\u001b[39m.\u001b[39mextend([a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m plan[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq]])\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:558\u001b[0m, in \u001b[0;36mTrajectoryOptimizer.optimize\u001b[0;34m(self, trajectory_eval_fn, callback)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     trajectory_eval_fn: Callable[[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    542\u001b[0m     callback: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    544\u001b[0m     \u001b[39m\"\"\"Runs the trajectory optimization.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39m        (tuple of np.ndarray and float): the best action sequence.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m     best_solution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    559\u001b[0m         trajectory_eval_fn,\n\u001b[1;32m    560\u001b[0m         x0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprevious_solution,\n\u001b[1;32m    561\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_last_solution:\n\u001b[1;32m    564\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_solution \u001b[39m=\u001b[39m best_solution\u001b[39m.\u001b[39mroll(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq, dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:172\u001b[0m, in \u001b[0;36mCEMOptimizer.optimize\u001b[0;34m(self, obj_fun, x0, callback, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_iterations):\n\u001b[1;32m    171\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_population(mu, dispersion, population)\n\u001b[0;32m--> 172\u001b[0m     values \u001b[39m=\u001b[39m obj_fun(population)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         callback(population, values, i)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:681\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act.<locals>.trajectory_eval_fn\u001b[0;34m(action_sequences)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(action_sequences):\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrajectory_eval_fn(obs, action_sequences)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:744\u001b[0m, in \u001b[0;36mcreate_trajectory_optim_agent_for_model.<locals>.trajectory_eval_fn\u001b[0;34m(initial_state, action_sequences)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(initial_state, action_sequences):\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m model_env\u001b[39m.\u001b[39;49mevaluate_action_sequences(\n\u001b[1;32m    745\u001b[0m         action_sequences, initial_state\u001b[39m=\u001b[39;49minitial_state, num_particles\u001b[39m=\u001b[39;49mnum_particles\n\u001b[1;32m    746\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:473\u001b[0m, in \u001b[0;36mModelEnvPushing.evaluate_action_sequences\u001b[0;34m(self, action_sequences, initial_state, num_particles)\u001b[0m\n\u001b[1;32m    469\u001b[0m action_for_step \u001b[39m=\u001b[39m action_sequences[:, time_step, :]\n\u001b[1;32m    470\u001b[0m action_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    471\u001b[0m     action_for_step, num_particles, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    472\u001b[0m )\n\u001b[0;32m--> 473\u001b[0m _, rewards, dones, model_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    474\u001b[0m     action_batch, model_state, sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    475\u001b[0m )\n\u001b[1;32m    476\u001b[0m rewards[terminated] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    477\u001b[0m terminated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m dones\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:140\u001b[0m, in \u001b[0;36mModelEnvPushing.step\u001b[0;34m(self, actions, model_state, sample)\u001b[0m\n\u001b[1;32m    124\u001b[0m (\n\u001b[1;32m    125\u001b[0m     next_observs,\n\u001b[1;32m    126\u001b[0m     pred_rewards,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     rng\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rng,\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m rewards \u001b[39m=\u001b[39m (\n\u001b[1;32m    136\u001b[0m     pred_rewards\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn(actions, next_observs)\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtermination_fn(actions, next_observs, rewards)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m pred_terminals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModelEnv doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yet support simulating terminal indicators.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:241\u001b[0m, in \u001b[0;36mModelEnvPushing.termination\u001b[0;34m(self, act, next_obs, rewards)\u001b[0m\n\u001b[1;32m    238\u001b[0m     cur_obj_pos_workframe \u001b[39m=\u001b[39m cur_obj_pos_to_goal_workframe[:, \u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal_pos_workframe_batch[:, \u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]\n\u001b[1;32m    240\u001b[0m      \u001b[39m# Calculate distance between goal and current positon\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     obj_goal_pos_dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(cur_obj_pos_to_goal_workframe, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    242\u001b[0m \u001b[39m# Default oracle observations\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     tcp_pos_workframe \u001b[39m=\u001b[39m next_obs[:, \u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main PETS loop\n",
    "num_test_trials = 10\n",
    "all_rewards = []\n",
    "evaluation_result = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "save_vid = True\n",
    "render = True\n",
    "\n",
    "if save_vid:\n",
    "    record_every_n_frames = 1\n",
    "    render_img = env.render(mode=\"rgb_array\")\n",
    "    render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(work_dir, \"evaluated_policy.mp4\"),\n",
    "        fourcc,\n",
    "        24.0,\n",
    "        render_img_size,\n",
    "    )\n",
    "\n",
    "for trial in range(num_test_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "\n",
    "    evaluation_result.append(np.hstack([trial, steps_trial, obs[0:3] + env.goal_pos_workframe, obs[7:10] + env.goal_pos_workframe, env.goal_pos_workframe, 0.0, done]))\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        action = agent.act(obs, **{})\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        if render:\n",
    "            render_img = env.render(mode=\"rgb_array\")\n",
    "        else:\n",
    "            render_img = None\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "\n",
    "        evaluation_result.append(np.hstack([trial, steps_trial, obs[0:3] + env.goal_pos_workframe, obs[7:10] + env.goal_pos_workframe, env.goal_pos_workframe, reward, done]))\n",
    "\n",
    "         # use record_every_n_frames to reduce size sometimes\n",
    "        if save_vid and steps_trial % record_every_n_frames == 0:\n",
    "\n",
    "            # warning to enable rendering\n",
    "            if render_img is None:\n",
    "                sys.exit('Must be rendering to save video')\n",
    "\n",
    "            render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "            out.write(render_img)\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    print(\"Terminated at step {} with reward {}, goal reached: {}\".format(steps_trial, total_reward, env.single_goal_reached))\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "if save_vid:\n",
    "    out.release()\n",
    "\n",
    "print(\"The average reward over {} episodes is {}\".format(num_test_trials, np.mean(all_rewards)))\n",
    "\n",
    "# Save data \n",
    "evaluation_result = np.array(evaluation_result)\n",
    "df = pd.DataFrame(evaluation_result, columns = ['trial','trial_steps','tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'dones'])\n",
    "pd.DataFrame(evaluation_result).to_csv(os.path.join(work_dir, \"evaluation_results.csv\"))\n",
    "\n",
    "# plot evaluation results\n",
    "evaluation_result_directory = os.path.join(work_dir, \"evaluation_result\")\n",
    "if not os.path.exists(evaluation_result_directory):\n",
    "    os.mkdir(evaluation_result_directory)\n",
    "\n",
    "for trial in range(num_test_trials):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    df.query(\"trial==@trial\")[\"tcp_x\"]\n",
    "    ax.plot(df.query(\"trial==@trial\")[\"tcp_x\"], df.query(\"trial==@trial\")[\"tcp_y\"], \"bs\", label='tcp psosition')\n",
    "    ax.plot(df.query(\"trial==@trial\")[\"contact_x\"], df.query(\"trial==@trial\")[\"contact_y\"], \"rs\", label='contact psosition')\n",
    "    ax.plot(df.query(\"trial==@trial\")[\"goal_x\"].iloc[0], df.query(\"trial==@trial\")[\"goal_y\"].iloc[0], \"x\", markersize=20, markeredgecolor=\"black\", label=\"goal position\")\n",
    "    ax.set_xlabel(\"x workframe\")\n",
    "    ax.set_ylabel(\"y workframe\")\n",
    "    ax.set_xlim([0, 0.3])\n",
    "    ax.set_ylim([-0.1, 0.1])\n",
    "    \n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(evaluation_result_directory, \"trial_{}.png\".format(trial)))\n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tactile_gym_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
